{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fra/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)];\n",
    "y = (iris.target==0).astype(np.int);\n",
    "per_clf = Perceptron();\n",
    "per_clf.fit(X,y);\n",
    "y_pred = per_clf.predict([[2, 0.5]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, Scikit-Learn’s Perceptron class is equivalent to using an `SGDClassifier` with the following hyperparameters: `loss=\"perceptron`, `learning_rate=\"constant`, `eta0=1` (the learning rate), and `penalty=None` (no regularization).\n",
    "\n",
    "Contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they just make predictions based on a hard threshold. This is one of the good reasons to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "There exist some weaknesses for Perceptrons, in particular the fact that they are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classification problem). Of course this is true of any other linear classification model as well (such as Logistic Regression).\n",
    "\n",
    "However, it turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tf.__version__,keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255., X_train_full[5000:]/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                   \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"];\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network just composed of a single stack of layers, connected sequentially\n",
    "model = keras.models.Sequential();\n",
    "#latten layer whose role is simply to convert each input image into a 1D array\n",
    "#if it receives input data X, it computes X.reshape(-1, 1). This layer does not have \n",
    "#any parameters, it is just there to do some simple preprocessing. Since it is the\n",
    "#first layer in the model, you should specify the input_shape:\n",
    "#Alternatively, you could add a keras.layers.InputLayer as the first layer,\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]));\n",
    "model.add(keras.layers.Dense(300, activation='relu'));\n",
    "model.add(keras.layers.Dense(100, activation='relu'));\n",
    "# one outut neuron per class\n",
    "model.add(keras.layers.Dense(10, activation='softmax'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x14abbdda0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1647b1668>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1647b1978>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1649b9240>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flatten_1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its `get_weights()` and `set_weights()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00750139,  0.05208854, -0.06803954, ...,  0.05454348,\n",
       "         -0.01365226,  0.013699  ],\n",
       "        [-0.01832159, -0.06986311, -0.00057953, ..., -0.04880586,\n",
       "         -0.02773645,  0.05937293],\n",
       "        [ 0.03042196,  0.06486151,  0.0046051 , ...,  0.0314239 ,\n",
       "         -0.06000398,  0.00875198],\n",
       "        ...,\n",
       "        [-0.02748642,  0.05775917,  0.01107278, ..., -0.06861664,\n",
       "         -0.04925692, -0.0730875 ],\n",
       "        [-0.00158256,  0.01777094, -0.00142841, ...,  0.0132817 ,\n",
       "          0.02905867,  0.05647796],\n",
       "        [-0.01021216,  0.06663913,  0.0617533 , ..., -0.01564594,\n",
       "         -0.06532486,  0.0603542 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sparse_categorical_crossentropy` loss because we have sparse labels and the classes are esclusive.\n",
    "\n",
    "If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. `[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]` to represent class 3), then we would need to use the `categorical_crossentropy` loss instead. If we were doing binary classi‐ fication (with one or more binary labels), then we would use the `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.7118 - accuracy: 0.7676 - val_loss: 0.5169 - val_accuracy: 0.8196\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.4843 - accuracy: 0.8313 - val_loss: 0.4326 - val_accuracy: 0.8544\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.4397 - accuracy: 0.8473 - val_loss: 0.4130 - val_accuracy: 0.8602\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.4111 - accuracy: 0.8546 - val_loss: 0.4145 - val_accuracy: 0.8540\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.3911 - accuracy: 0.8615 - val_loss: 0.3808 - val_accuracy: 0.8680\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.3752 - accuracy: 0.8685 - val_loss: 0.3704 - val_accuracy: 0.8726\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.3620 - accuracy: 0.8710 - val_loss: 0.3796 - val_accuracy: 0.8648\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.3522 - accuracy: 0.8754 - val_loss: 0.3486 - val_accuracy: 0.8798\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.3414 - accuracy: 0.8791 - val_loss: 0.3530 - val_accuracy: 0.8760\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3328 - accuracy: 0.8812 - val_loss: 0.3371 - val_accuracy: 0.8806\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3243 - accuracy: 0.8844 - val_loss: 0.3381 - val_accuracy: 0.8784\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.3163 - accuracy: 0.8861 - val_loss: 0.3332 - val_accuracy: 0.8824\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.3091 - accuracy: 0.8889 - val_loss: 0.3409 - val_accuracy: 0.8774\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.3028 - accuracy: 0.8892 - val_loss: 0.3300 - val_accuracy: 0.8816\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2961 - accuracy: 0.8933 - val_loss: 0.3229 - val_accuracy: 0.8826\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2900 - accuracy: 0.8957 - val_loss: 0.3267 - val_accuracy: 0.8836\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2845 - accuracy: 0.8974 - val_loss: 0.3117 - val_accuracy: 0.8864\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2786 - accuracy: 0.8983 - val_loss: 0.3252 - val_accuracy: 0.8848\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2732 - accuracy: 0.9025 - val_loss: 0.3096 - val_accuracy: 0.8910\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2676 - accuracy: 0.9031 - val_loss: 0.3384 - val_accuracy: 0.8740\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.2635 - accuracy: 0.9038 - val_loss: 0.3039 - val_accuracy: 0.8906\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2580 - accuracy: 0.9071 - val_loss: 0.3054 - val_accuracy: 0.8900\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2545 - accuracy: 0.9075 - val_loss: 0.3031 - val_accuracy: 0.8928\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.2491 - accuracy: 0.9098 - val_loss: 0.3101 - val_accuracy: 0.8900\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 3s 51us/sample - loss: 0.2457 - accuracy: 0.9112 - val_loss: 0.3021 - val_accuracy: 0.8876\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2415 - accuracy: 0.9135 - val_loss: 0.3043 - val_accuracy: 0.8922\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2375 - accuracy: 0.9136 - val_loss: 0.2976 - val_accuracy: 0.8934\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.2347 - accuracy: 0.9154 - val_loss: 0.3440 - val_accuracy: 0.8720\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2302 - accuracy: 0.9175 - val_loss: 0.3047 - val_accuracy: 0.8934\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2259 - accuracy: 0.9183 - val_loss: 0.3092 - val_accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "hist_obj = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid));\n",
    "#history  = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set was very skewed, with some classes being overrepresented and oth‐ ers underrepresented, it would be useful to set the class_weight argument when calling the fit() method, giving a larger weight to underrepresented classes, and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss. \n",
    "\n",
    "If you need per-instance weights instead, you can set the sam ple_weight argument. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the `validation_data` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEzCAYAAAALosttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl8nWWd///Xde6zn+z70i0t3UtbaEsXoC2ySNGRRR3AGQRBUAH5IjouUFdmREHnJyKIiKCMoiDCyCiLgARooTt0b0r3Zk+Tk+Xs2/X74z5ZmzZJmyYnzef5eJzHvZ77XOduk3eu5b5vpbVGCCGEEKnDMtwFEEIIIUR3Es5CCCFEipFwFkIIIVKMhLMQQgiRYiSchRBCiBQj4SyEEEKkmD7DWSn1hFKqXim17RjblVLq50qpPUqpLUqpswe/mEIIIcTo0Z+a82+BS4+zfQUwOfm6BfjlyRdLCCGEGL36DGet9dtA03F2uRx4SpvWAFlKqeLBKqAQQggx2gxGn3MpcLjLcmVynRBCCCFOgHUQjqF6WdfrPUGVUrdgNn3jdDrnjRs3bhA+fvRIJBJYLDKGbyDknA2cnLOBk3M2cKPxnO3evfuI1jq/P/sORjhXAmO7LI8BqnvbUWv9GPAYwNSpU3VFRcUgfPzoUV5ezvLly4e7GCOKnLOBk3M2cHLOBm40njOl1MH+7jsYf7a8CHw2OWp7EdCita4ZhOMKIYQQo1KfNWel1B+B5UCeUqoS+C5gA9BaPwq8BFwG7AECwOdOVWGFEEKI0aDPcNZaX9vHdg3cNmglEkIIIUa50dUbL4QQQowAEs5CCCFEipFwFkIIIVKMhLMQQgiRYiSchRBCiBQj4SyEEEKkGAlnIYQQIsVIOAshhBApRsJZCCGESDESzkIIIUSKkXAWQgghUoyEsxBCCJFiJJyFEEKIFCPhLIQQQqQYCWchhBAixUg4CyGEECnGOtwFEEIIIVJCIgGxkPmKBs1XLNhlPgSxMCRikIiDjnfO92fdAEg4CyGEOHXiMYgGuryCEAmQ2bwDDjoAbe6nk9NjLXddp+MQi5jBGQt3hmbPabTr9i7b2oO2ZwDHQqfuPChjQLtLOAshxOnuGAFJLATxCMSjEA8np5Eu6yJmoB21PtwZjtEgRPydQRdtnw+Yn5GI9lqkswA+OEXf12IFqxOsjh7T5LwzA6yFYHOCzQVWlzltf1ldyW1u8z02d+e+ht08vsUKFsMM3a7Lva1TFlAKvqf6/RUknIUQ4lRJJCDig3ArhFq7T9vnI37QiV5e2pyij7E90dkM2x6G7cHbNYCjxw7IAVGGGUyGHQybObW5wO42w8vuBk9+Z8DZPcn55Pb2ebsbrC42b9vBnDlzksduDy3Vx3JynbIcHbw2lzlvOMAY+dE28r+BEGJ00toMuEATBJsg7Ev27SWS03j3qdZHr0vEkyF3rPWx4++biIGOM73qIFQ93EsAt9GtWfZYVHvtqutL9Zge42V1JIPPA/Y08BR0D81jBaTN3SXMbN2D1+roHsKG3awBDiJvtR0mLR/UY55OJJyFEEMr0R56PV7xaPewPWrq7b4c9A54kM0JUZZkM6XRZWrptpweA4xCs7nUMxEcGeZ816kjPTmf2X2d3dOjZiiEhLMQo0ciYfYRRgJmv2CkSx9kl3UlVVvg3a3JwTPhzn7JjgE1va0PmX2Q8XCXUarJwO26nIjRr5pkV4YD3DngyjGnBdM657tOHWlmH58ywGLpI1T7DtzOad/Bua68nOXLl5/QP4sQvZFwFmKkiEXM2mLQ21lzDDQdvRxqNvsxewvhfpgC8GGXFe39ekbXPj5HZ1+fPQ3ced2bQi1dB8S0D5Sx9Vjuud1q1ih7Bq/NfdyA1FqTaGsDpTDS00/qFJ8IrTUkEmitUSdZA9axGIlAgITf3zk97nwQZbNh8bixeDxY3J7OeY8Hi/voeeVwnHQ5B/SdolFiXi9xr5d4UxOxpibi3mbc27fRsGMHxGLoaBQdjaHb52NRdDTafVs02rHd4vGQsWIFGR+9BIvHc+q/g9YEN22i5X//l2hVFdbiYmwlJdiKS8xpaQm2wkKU3T5onynhLER/JeLJ0POZ/ZuR9pc/udzWZd6X7I/UmAN6ug7uaV+X6DJP9+3xcJfgbTbDN+I7dtksVnBlm4HmygJnFmSUdPY12j1d+ho9XQbxHL1+9bpNnLv0I2bwGrZhbXLVWhNvbiZWV0e0tpZYbR3RulpiNbXmtLaOaF0dOhAAiwXX2WeRfsEFpF1wAfayslMWQvHWVvyrV+MrfwvfO+9Q2NTELjDPlWGgLBawWMxpl+XetqF1R+DqcLh/BVAKi9uNcrsgEiURCJhh1h+G0RnabjcWlwuLy4VKTi0uF8rtwuJKbnO3b3NjcTk797Xbibe2doRtvKmJmLeJeJO3c97bTKK1tddipANH2r+OzQY2G8pmQ1mt5qt93mYDW/uyDWW3Ezl0kJq776b2P/+TjI9+lKyrrsQ1f/6g/3tHa2po+euLtLzwApGDB1FuN45Jkwh9+DbxhiPdd1YKa36+GdYlZnhbS0qwFRdjKynFVloyoM+WcBanh1gkORCnxRyEkxyUU1C3ATYd7n6NY7cm2mRzbPtyvEuTbTTUPYD7WfMEzMCz2JKDTbsM7EF1DvBpnz9qO2YN1JUD6cVQOLN78Lpzksvt67LNvssB/GLSWpu1klgMHY+bv9jjcXQoRsIbIbSvkoTfT9zvRwcCxI9Zg+tey9PhsPkL1eFA2e3Jl/kL1WLvus6OctixdF222Yg3t3QJXXN6VGBZLFgLCrAVFuKYMoW0pedjLSwi3tqCr/wt6h/4CfUP/ATb+HGkLzeD2j3vbPOX/AnSWhPZuxffW2/hK3+LwKZNEI9jZGbiWbqUamDCuHHoRBwS5sAzHU+Y51QnIJ4wt8XNUdYd2xIJUHQPyx41XcPjQbndGF3WK5fLDPeuZYxEkv9OARKB5L9Rx/xxauOhIDoQJO71Eq2uJhEMoANBEqEQOjSA635tNqxZWRg5ORg52bhKZmFkZ2PkZGPNycHIzsHIzsaak42Rk8OqjRtZ9pGPmH+wDDBUtdYE33+f5uefp+2ll2l54QVs48aRdeUVZF5+ObaSgQVhV4lwmLbXX6fl+Rfwv/suaI17wQJyv/CFbjX1RCRCrKaGaHU10er2aTXRmhqC27bT+trr0N8/mHqhtB5g/88gmTp1qq6oqBiWzx6pyoepXysRDkMigXI6T+4vU63NPshY0Ay+nnfeiQbR0SCJtpaOJrB4SysqEUYRxkIQlQhg0QEsCT8q3oaKtaEibf2+eYCOQzxqIR5zEo85iMfsxGM24lGDeMQgHrGQSGZ0IgpGmgMjw401Mw1rdiZGdibW3BysufkYBQUYOQUoZ3rnwB57mjntY2SrjseJt7QQb27ufHk75xN+fzI4Y8kQjXcuR5OBGot1D9ge23QsCh3vi0M02vm+ePyE/xmVw3HMMFFOh9n8GImiIxF0OGxOIxF0NEIiHOlcbt/e9ReY1YqtoABrURG2okKshclpUXFyWoQ1NxdlPXa9IlpdTVt5Ob43ywmsWWM2g6ank3b++aRdsJy088/HyMrq83smwmECa9eateO33iJaVQWAY9o00pYtI23ZMlxzZqMMY9h+Nk81nUigg0ES7a9AEB0y53U4jCUjwwzenBwsaWkD+v0wWOcsEQjQ9tprND//AoG1a0EpPIsXkXnlVaRffBEWp7PPY2itCW3dSvPzz9P60sskWluxlhSTdcUVZF5xBfZx4wZcLp1IEDtyhFh7aFdXk3fzzRu11vP7834J5xFkqH4B6GiU4NZt+N97F/977xH8YDPEYmbfVkY6RpoHI82J4XZicVsxHBbzagx7HIs1hmFEMIwghgpgoQ0VbSPujxD3R4iFFPGwhXjYQiw5jYcsxCIW4iGDeNiCTgzgDwAFymZgcdiwOGxmcDidKJcTi8tNS3MrHmUQb/WRaG0jEThO7ddiwcjM7Hgpl8tsUm08QrzJ23ugGYZZM8jNw5qbi5Gbk5zPQUejHWEb6xrCzS0kWlqOXQ6rFcPjMZv5DMMMIqthNul1LCeb/gwDZbOCkVy2Gl3m299n7Xxvz32tVpTRfd+K/fuZOW9e7zU5t/ukaqC90VonAz1iHr9HjfBkJPx+/O+9R9ubb+Irf4t4YyMYBu6zziIt2fztmFjWsX+0pqajduxfswYdCqFcLjyLF5O2dClpy5ZiKy4+6nNO13A+lU7FOYtUVtLyv3+l5YUXiFZVYUlLI+Oyy8i66kqcc+Yc9cdDrKGBlhdfpPmFF4js2YtyOEi/5BKyrroS98KFg/p/EUApJeF8OtHxOPHmZlavWcOyj31scA4aj3YMINL+I0R278S/YTP+zbsJVNSQCJuXqDgLbXhKNBZLkEQgTDyszJpnxEIioohHkvPRgf8ntrjsGBkejMx0rFkZGFlZGNlZ5l/iuXkYeQUY2XlgsZs1rlCQRDBkNsMFQ8lmt17WBc0muUQwSKvPR8748Z2hm5WJpSOAs8zPzDKXLR7PMX8YdSJh1nQbG4kdaTQDu7GJWGP3+XhjI7HGxo7mQIvbnfyMY7yyj1430BrIYDtdg0YnEoS2bjWD+s1ywsnfP/bx43HNn0do2/aOdbYxY8za8fJluM85B4vDcdxjn67n7FQ6ledMJxIE1q2n5YXnaX31H+hQCPvEiWReeQWZl11GcNt2Wl54Ad8770A8jmvuXDKvupKMFStO6aBCCecUlwiHu4xc9BL3NiUHUHjNwRTe5PqmJnO/lpaOe8pa0tOwlxZhLynAVpiLvSATe0E6tlwPVrdCxXtcKhPxm/NhX+c1ooEmot42/HUO/LUO/HUO4iGzGdaWFsNTEsczzonnjGyMnLzOUbPOrM7BRj2m2p5OIqyJt7aar5ZWEq0txFtb0ZFo976nHLPvyTKIIxuPZTh+aWqt0YGAOYBlCL7jYBstQROtqqLtrbfwvVlO8P33cc6Y0RHI9okTh6WJdjQZqnMW9/loe+UVmp9/geCmTR3rrQUFZF5+OZlXXoFj4sRTXg4YWDjLgLBTLFpVhX/9egLr1hN8/31idXXHblq1WDAy0jDSnFjdBo70BEaOwmqxYugWdCxKxOcn2nyE4GErrQEDdOcvEGVobJ4Y9vQY9jSNLcuKPceOPceFJd1D8IgD/+Fs/PutROrNQQ1GhgfPwul4zpmP59zzsU2abt5FaIAUYLjByM4+kdN0WlFKoYbg8g5xcmylpeR85jPkfOYzw10UcQoZaWlkfepTZH3qU4T378f3xhs4pkzBs2TJcccuDLfULdkIpLUmWllJYN16AuvXE1i3jmh1NQCWjAzcc2fhmTsZqyOOYQtjKB9W7cWI12NEqzEswe4Dbl05kDUOsmZD1nj21rUxadqZHZe/aIuLaHOQSH0LkbpmorVHiFTXE62swn+wCl3RPkgqmHyBcrtxL5hP1ucW41myGMfkyYPeryKEEKnIUVaG4/OfH+5i9MtpGc46FjNHydXXE6uvJ1pfT6zOnI81NKAcDmyFhUePCC0s7NfIvo7P0ZrowYPJmvE6AmvXEKs3r30z0hy4J2SQMzMfd3YLDus+VGJX55ujgDs3Gb6zIOsyyByXXB4HWWPNEcBdHC4vZ9I5yzuWFWBPvnorW6y+gejhQ0QOHiLe7MU1Zw6u2bNHZFOrEEKMJiMunLXWRPbsMYemJ8M3Vt9ArK7ODOKGeuJHGrs8CzTJMLDm52PNz0eHwwQ2bOh1xKyRlWWGds/wLsjD6k5AazWB9esIbN5FoKKaWFvEfJ8jjrsggnteGE9BBHtGDJVWkAzasyDzE8n58eY0c4x5u8FTRCmFrbAAW2EB7vn96uIQQgiRIkZUOEcOHaL2B/fiX7Wq23ojJwdrQQHWgnycM2dgzS8wlwvNqa2gACMnB2V0v/Y0EQgQrasz7z5UU0Ps0B6ih/cRq6kmum8rwQ3vEvf3fhG51RXHXWLgXpSDe2YZ9jOmoLKTtd7MZPja+l8LF0IIIdqNiHBORCI0Pv44jY/+CmW1UvAfX8N19tnmzQry8/vfTKs1NOyC+p3QfBCL9yCO5oM4vAeh5bB556g0YHLylV5MIm0cMUsh0Xg2sagb7cjCveh8bDMWoKzSPCyEEGLwpXw4+997j9rv/4DIgQOkX3ophd/6JrbCwv4fINgM+9+CD1+DPW9AW3XnNlcOZI+HojNh+sfNJufs8ZA1oaPma+HY/bpCCCHEqZCy4RxraKDux/fT+re/YRs7lrG/foy088/v+42JBNRugT3JMD68zrxnoyPTfLD3GRdBydlmCDuG/gk2QgghRF9SLpx1PI73T3+i4WcPokMh8m79Erm33HL8UdT+Rtj3plk73vsG+BvM9cVz4byvwOSLoXQ+GCn3dYUQQoijpFRaBbdtp/Z73yO0bRvuxYso+vZ3ut33tkMiDlWbYM/rZg25ahOgzWbqMy40a8eTLoS0/CH/DkIIIcTJSolwjre10fDgz/E+/TRGTg4lDzxAxsc/1vvt8/a/A3++AQJHzMfslc6H5d9KNlfP7fNpQEIIIUSqG9Zw1lrT9vLL1N53H/EjjWRfey35d/4/jIyM3t9Qtx3+9BlIL4LL7oeJF5j3fBZCCCFOI8MWzioW4/BNn8f/7rs4Z85k7COP4DrzzGO/ofkw/P6T5vNyr3vBHE0thBBCnIaGLZyt1dUEt2yhcOVKsq+95qgbhHQT9MIfPmU+YenGVySYhRBCnNaGLZwTbjcTX/o7toKC4+8YDcGf/g0a98J1z0PhzKEpoBBCCDFMhi2c43l5fQdzIgEv3AIHV8MnfwNlS4emcEIIIcQw6tezApVSlyqlKpRSe5RS3+xl+zil1JtKqfeVUluUUpeddMm0hle/BTv+Cpf8F5z5qZM+pBBCCDES9BnOSikDeBhYAcwArlVKzeix20rgWa31WcA1wCMnXbJ3H4K1j8KiW2HJ7Sd9OCGEEGKk6E/N+Rxgj9Z6n9Y6AvwJuLzHPhpov/4pE6jmZGx9Dl77Nsy80qw1CyGEEKOI0j2fe9xzB6U+BVyqtf58cvk6YKHW+vYu+xQD/wCyAQ9wkdZ6Yy/HugW4BSA/P3/es88+e9TnZXm3MHvL92nNmMaW2d8lYcgjJ9r5fD7S0k7dM6BPR3LOBk7O2cDJORu40XjOLrjggo1a6/n92bc/A8J6uU0XPRP9WuC3WuufKqUWA/+jlJqltU50e5PWjwGPAUydOlUvX768+1Fqt8GTD0DeZLJu/DtLXVn9+Q6jRnl5OUedM3Fccs4GTs7ZwMk5Gzg5Z8fXn2btSmBsl+UxHN1sfRPwLIDW+j3ACeQNqCTNh81rme1p8O/PgQSzEEKIUao/4bwemKyUKlNK2TEHfL3YY59DwIUASqnpmOHc0O9SBJrMu39FAmYwy01GhBBCjGJ9hrPWOgbcDrwK7MQclb1dKfUDpdQnkrt9FbhZKbUZ+CNwg+6rM7tdNGTeL9u7H675g9xkRAghxKjXr5uQaK1fAl7qse47XeZ3AOcO+NMTcXj+Zjj0HnzqSSg7f8CHEEIIIU43/boJySnzyrdg54vw0R/CrKuGtShCCCFEqhi2cLZHmmHdr2Dx7bD4tuEqhhBCCJFyhi2cHeEjMPMquPje4SqCEEIIkZKGLZzjhguufBQsw9uyLoQQQqSaYUvGoLsErI7h+nghhBAiZQ1bOOtebzwmhBBCCGlTFkIIIVKMhLMQQgiRYiSchRBCiBQj4SyEEEKkGAlnIYQQIsVIOAshhBApRsJZCCGESDESzkIIIUSKkXAWQgghUoyEsxBCCJFiJJyFEEKIFDNs4eyP6uH6aCGEECKlDVs4N4U0WktACyGEED0N3/OcNRxuCg7XxwshhBApa1j7nNfsaxzOjxdCCCFS0rCFs6EknIUQQojeDFs4Ow3Fmn2N0u8shBBC9DB84WyF6pYQlV7pdxZCCCG6GsZwVgC8J03bQgghRDfDFs42C+R47NLvLIQQQvQwrKO1F03MYe2+puEsghBCCJFyhjmcc6lqDnK4KTCcxRBCCCFSyrCG88KyXEAuqRJCCCG6GtZwnlyQlux3lqZtIYQQot2whrPFolhYliM1ZyGEEKKLYX9kpPQ7CyGEEN0NezgvnJgDwNr90rQthBBCQAqE85SCdLLdNmnaFkIIIZKGPZzNfudc1u6XcBZCCCEgBcIZzJuRHG4KUumVfmchhBAiJcJ54UTzeme5W5gQQgiRIuE8tTCdLOl3FkIIIYAUCef2651lxLYQQgiRIuEM5vXOh5oCVDXL852FEEKMbikTzu332V4rTdtCCCFGuZQJ52lF6WS6pN9ZCCGESJlwln5nIYQQwpQy4Qxmv/PBxgDV0u8shBBiFEupcO68z7Y0bQshhBi9+hXOSqlLlVIVSqk9SqlvHmOff1VK7VBKbVdKPX0ihZlelGH2O++Vpm0hhBCjl7WvHZRSBvAwcDFQCaxXSr2otd7RZZ/JwLeAc7XWXqVUwYkUxmJRnFOWIzVnIYQQo1p/as7nAHu01vu01hHgT8DlPfa5GXhYa+0F0FrXn2iBFk3M5UBjgJoW6XcWQggxOvUnnEuBw12WK5PrupoCTFFKrVZKrVFKXXqiBVpYlux3lvtsCyGEGKX6bNYGVC/rdC/HmQwsB8YA7yilZmmtm7sdSKlbgFsA8vPzKS8vP+rACa1xW+H51dvIavmwH8UbPXw+X6/nTBybnLOBk3M2cHLOBk7O2fH1J5wrgbFdlscA1b3ss0ZrHQX2K6UqMMN6fdedtNaPAY8BTJ06VS9fvrzXD1xyaAN7G3wca/toVV5eLudkgOScDZycs4GTczZwcs6Orz/N2uuByUqpMqWUHbgGeLHHPv8LXACglMrDbObed6KFWjQxh/1H/NS2hE70EEIIIcSI1Wc4a61jwO3Aq8BO4Fmt9Xal1A+UUp9I7vYq0KiU2gG8CfyH1vqEh1wvan++s4zaFkIIMQr1p1kbrfVLwEs91n2ny7wG7kq+Ttr04gzSnVbW7Gvk8rk9x54JIYQQp7eUukNYO6P9PtsyYlsIIcQolJLhDGbT9r4jfupapd9ZCCHE6JKy4dz+fGd5hKQQQojRJmXDeUZJBukOK2ukaVsIIcQok7LhbMh9toUQQoxSKRvOkOx3bvBTL/3OQgghRpGUDuf25zuv2S9N20IIIUaPlA7nGcXt/c7StC2EEGL0SOlwthoWFpTlsFbCWQghxCiS0uEM5n229zb4qW+TfmchhBCjQ8qHc/v1znK3MCGEEKNFyofzzJIM0qTfWQghxCiS8uFsNSwsmJDNWhmxLYQQYpRI+XAG83rnPfU+GtrCw10UIYQQ4pQbEeG8UJ7vLIQQYhQZEeE8qyQDj92QfmchhBCjwrCFsz/h7/e+ndc7S7+zEEKI09+whXNTrIn9Lfv7vf+iibl8WO/jiE/6nYUQQpzehi2cFYqVq1cST8T7tf/CMvM+21J7FkIIcbobtnDOseawpWELT+14ql/7zyrNxGM3ZFCYEEKI096whbPb4ubCcRfyi/d/wd7mvX3ubzMszJ+QI4PChBBCnPaGdbT2ykUrcdvcrFy1klgi1uf+iybmsrtO+p2FEEKc3oY1nPNcedyz6B62NW7jt9t/2+f+7c93Xid3CxNCCHEaG/brnC+dcCkXj7+YRz54hA+9Hx533zNLM3HbDXmEpBBCiNPasIczmM3b6fZ0Vq5eSTQRPeZ+nf3OUnMWQghx+kqJcM5x5rBy0Up2NO7gia1PHHffRRNzqKhro6o5OESlE0IIIYZWSoQzwMXjL2bFhBU8uuVRKpoqjrnfJTOKcNosXPXIajYd8g5hCYUQQoihkTLhDHD3wrvJtGeazdvx3pu3zyhI4/kvnYvDanD1r97j92sOorUe4pIKIYQQp05KhXOWM4vvLP4Ou5p28eutvz7mfjNKMvi/28/j3DPyWPm/2/j6c1sIRft3pzEhhBAi1aVUOAN8ZNxH+PjEj/PrLb9mR+OOY+6X6bbxxPULuOPCyfx5YyWffvQ9Kr2BISypEEIIcWqkXDgDfPOcb5LtzD5u8zaAxaK46+IpPP7Z+Rw44udfHlrFqg+PDGFJhRBCiMGXkuGc6cjku4u/y4feD3l0y6N97n/RjEJe/PJ55Kc7+OwTa/ll+V7phxZCCDFipWQ4Aywbu4xPTPoEv9n6G7Yf2d7n/mV5Hl649VwuO7OYH7+yi1v/sAlfuO9bggohhBCpJmXDGeAb53yDXGcu96y6h0g80uf+HoeVh649i5Ufm84/dtRx+S9WsafeNwQlFUIIIQZPSodzhj2D7y35Hntb9vLIB4/06z1KKT5//kR+f9NCmgNRrnh4Na9sqz3FJRVCCCEGT0qHM8D5Y87nqslX8eT2J9nSsKXf71s8KZe/3XEekwrS+OLvN3L/K7uIJ6QfWgghROpL+XAG+Nr8r1HgLmDl6pWEYqF+v68408WzX1jEZxaO45Hyvdzw5Dq8/r6bx4UQQojhNCLCOd2ezveXfJ/9Lft5+IOHB/Reh9Xgh1eeyY8/eSZr9zfx8YdWySMnhRBCpLQREc4AS0qW8Okpn+Z323/HB/UfDPj9Vy8Yx3NfXAzAv/7qPa77zVq5N7cQQoiUNGLCGeCr879KsaeYlatXsrlh84CvZZ49JovX7lrKt1ZMY3t1K1c98i43PLmOzYebT1GJhRBCiIEbUeHssXn4z/P+kzp/Hf/+0r+z4vkVPLjpQSqaKvod1G67lS8sm8Q7X7+Ab1w6jc2Hm7n84dXc+Nv1bK1sOcXfQAghhOibdbgLMFALihbwz3/9J28ceoNX9r/Ck9ue5PGtjzMpcxIrylawomwF4zLG9Xkcj8PKl5ZP4rrF4/nduwd47O19/MsvVnHR9ELuvGgys0ozh+DbCCGEEEcbceEM5gCxK864givOuILGYCOvHXyNl/e/zC8++AW/+OAXzMqdxaVll3LphEsp9BQe91hpDiu3XXAGn108nidXH+Dxd/bx8Yfq+OjMQu68aArTizOG6FsJIYQQphEZzl3lunK5Zto1XDPtGmp8Nbx64FVe2v8SP9nwE3664afMK5zHirIVXDL+ErKcWcc8TrqbAdSxAAAgAElEQVTTxh0XTub6JRN4cvV+frNqP69uf4cVs4q486IpTC1KH8JvJYQQYjQb8eHcVXFaMTfMuoEbZt3AgZYDvLz/ZV7a/xL3rrmX+9bex6KSRVxWdhkTMiYQSUSIxCNEE1Gi8WjHciQRoaA0ym2Xh3hvfy3vHHqDfz4dpSzfwZljPJRlF/Jv0/+NTIc0ewshhDg1+hXOSqlLgQcBA3hca/2jY+z3KeDPwAKt9YZBK+UJmJA5gS/N/RJfnPNFKrwVvLT/JV7Z/wp3r7p7QMdR2eDESmXMoHKfgbIGeWr703x1/p18cspVWNSIGlMnhBBiBOgznJVSBvAwcDFQCaxXSr2otd7RY7904A5g7ako6IlSSjEtZxrTcqZx59l3su3INprDzdgsNuyGHbvFjs2wdZvaDXvHdpvFhlKKJn+EX7+zjz9sWkNL9l/4wZrv8/P1v+eOOf/Bp2YtQSk13F9VCCHEaaI/NedzgD1a630ASqk/AZcDO3rsdy9wP/C1QS3hILIoC7PzZ5/Qe3M8dr5x6TTuvGgyb1VcymPvP8eu8NP8YNMXuW/VIq6c8HmuOXsGkwulb1oIIcTJ6U+bbClwuMtyZXJdB6XUWcBYrfXfBrFsKclhNbhkZhHP/fvtvHH1SyzJ+yRR13qeqbmdy377Yy59sJxfvbWX6ubgcBdVCCHECKX6unmHUurTwEe11p9PLl8HnKO1/nJy2QL8E7hBa31AKVUOfK23Pmel1C3ALQD5+fnznn322cH8LsOmNlrLn448x95IBUa0mLaqT5AIljEl28LiEivzC62k2U++2dvn85GWljYIJR495JwNnJyzgZNzNnCj8ZxdcMEFG7XW8/uzb3/CeTHwPa31R5PL3wLQWt+XXM4E9gK+5FuKgCbgE8cbFDZ16lRdUVHRnzKOCFprXj/0Ovevv59afy1nuJfSVHkJB+us2AzFsikFXD63hOVT80l32k7oM8rLy1m+fPngFvw0J+ds4OScDZycs4EbjedMKdXvcO5Pn/N6YLJSqgyoAq4BPtO+UWvdAuR1+fByjlFzPp0ppbh4/MWcV3oej299nCe3PYmtYANfXHgDkaZz+fuWel7fWYfVopg3PptlU/NZOjmfGcUZWCwymEwIIUSnPsNZax1TSt0OvIp5KdUTWuvtSqkfABu01i+e6kKOJC6riy+f9WUun3Q596+/nz98+AhlmS/z089+E1vkLMp3N/BWRQP3v1LB/a9UkJfmYOmUPJZNyef8yfnkeOzD/RWEEEIMs35d56y1fgl4qce67xxj3+UnX6yRb1zGOH5x4S94u/JtfrTuR3zpjS9w0biL+Nj0j/GZJVOxkcvqD5t4a3cDb+6q5/lNVSgFs0szWTYln2VT85kzJgurcfLXUccSMfxRv9w4RQghRojT6g5hqWjpmKUsLF7IU9uf4tdbf83rh14HzCdsTcmewpSyKXzr7KlYY6UcqM5g9Z5WfvHmHn7+zz1kOK2cN9msVS+dkt/nZ0XiEQ60HmBfyz72Ne9jb/Ne9rXs40DrAWKJGCsmrODLZ3+ZseljT/XXFkIIcRIknIeAw3Bw8+ybuW7Gdexp3kNFUwUV3goqmir4276/4Y8+A5jXYY8vHs8npk7GFi/lSGMeGw74eWlrDaAo8iiWN25h9lgXBbmtBKnuCOJ9Lfs43HaYuI53HGtM2hgmZk1k2ZhlxHWcZyqe4bVDr3HN1Gv4wuwvHPde40IIIYaPhPMQclqdzMqbxay8WR3rtNZU+ao6AntX0y52ebdT5fuHuUMxlI7LIsMYR2NrhL976/lbmxelzFH2CoNsWwmTsiZy8fhLmJx9BhMzJzI+YzxOq7Pb51834zoe+eARnt71NH/d81duOvMm/m36vx21nxBCiOEl4TzMlFKMSR/DmPQxXDj+wo71rZFWdjftpsJbwW7vbnY17SIRb2VZ6TlkWscQDuRReySLnYdtHGyKchB4z2Zw1rgs5k9QzB/fxlnjjG6XbRW4C/jeku9x3Yzr+NnGn/GzTT/jj7v+yJfP+jIfn/hxDIsxDGdACCFETxLOKSrDnsH8ovnML+q8JK68vJzly5YftW9tS4gNB5vYcMDL+gNN/OKfH5LQYFEwvTiDBRNyOHt8NjOK05mQ62FS1iQeuvAh1teu56cbfsrK1St5asdT3DXvLs4tPXcIv6UQQojeSDifBooynXx8dgkfn10CgC8c4/1DXtYf8LLhQBPPrD/Mb989AIDdsDAx38PUonSmFmVz08T/j7rx6/j97l/yxde/yOLixXxl3leYnjt9GL+REEKMbhLOp6E0h5XzJ5vXTQNE4wkqatvYXddGRV0bFbVtrN/fxF8/qE6+w4rHfjv5YzaxoeZl/vVvV7Mw/yL+37w7mFUwXp64JYQQQ0zCeRSwGRZmlWYyq7T7dc6toSgf1rVRUeszg7s2j5bDcwm7/sGaxJuseelNDN/5THF8gplFxcwsyWBmSSaTCtwEYz6aw800h5vxhryd82EvLeGWjnXekBerxcrC4oWcW3Iu84vm47K6hulMCCHEyCDhPIplOG3MG5/DvPE5Heu01hzxfZQ1h/bw+4rH2GF5k916LTurC3i21o/aHEAZgY7R4j3ZLDayHdlkObPIdmQzNWcqvoiP53Y/xx92/gG7xc78ovmcW3Iu55WeR1lmmdTMhRCiBwln0Y1Sivx0B/8ycyb/MvNBKpoq+NWWX9EcasaqyohH3fiDDrxtdmq9FnwBBzrugYSb8RkFzCjOY1ZBVrKWnUFumgOAUCzEprpNrKpexeqq1Tyw4QEe2PAAxZ5izi09l/NKzmNh8ULS7KPrKTVCiKGT0AleP/g6c/LnUOgpHO7iHJeEsziuqTlT+e/l/93rNq01NS0htle3sr26he3VrXxwuIW/b63t2Kcow8n04nQm5qdRllfKefk3cv3UO4ipRt6teZfVVat5ef/LPLf7OazKypyCOZxXeh7nlpzL1JypWNTJ3750KASiAQ63HeZw22Eq2yqp9FWS48xhes50pudOp9BdKC0EQgyjQDTA3avu5o1Db5BuT+fbi77NirIVw12sY5JwFidMKUVJlouSLBcXz+j8K7Q5EGFHdWtHaO+qbeO9fY2EoomOfTx2g7L8IibmXc+/5t+CxXWII/Et7GxZz4ObHuTBTQ+S68xlUcki0mxpxBIx86VjxBPxjuWojnZbjmtzPpqImuuCMf742h/JceZ0e+W6crst93UjFq01jaHGjvBtD+L2V1Ooqdv+6bZ0fFEfGrP5P8uRxfSc6UzLncaMnBlMy5nGuIxxI+aPj9OVP+rnDzv/wKsHXuWyssv47IzPYjNO7JGuInVVtlVyx5t3sLd5L7fOuZVV1av4+ttf581Db3LPontS8rkDEs5i0GW57Sw5I48lZ3Q8SZREQlPTGmJ/g599R3zsa/Cz74ifTYe8/N+WIOZjxWcCMynICpOTdwCsFZQffA9UHIdhw261YVUGVou186XMqWExcFgdWC1WbMqGYTH3q6qrwhfxcbD1IE2hJoKxYK9ldlvdZlC7kuHtzMVldVHtq+awzwzkru9VKIo8RYxNH8sFYy/ouJHM2PSxjE0fS4Y9g0A0wG7vbnY27WRX0y52Nu7kf3b8D7FErOMzp+VM63jNyJ3BxKyJ2CwSDqdaIBrg6V1P89vtv6Ul3MKkzEn8bNPPeHHvi9yz8B7OKT5nuIsoBsn62vXcVX4XcR3nlxf+kiWlS7h59s38ZutveHTzo2ys28i9597LktIlw13UbiScxZCwWBSlWS5Ks1ycNzmv27ZQNM7BxgD7GnzsO+JPBnch+6pm0RKMduxnMxTjcz2MyfMwMd/DxDxPsrncQ67H3muzcc8HugeiAbxhL03BJppC5qsx1Ngx3xRsosZXw/Yj2/FH/RR7ihmbPpaFRQu7hW9pWil24/iP93Tb3MwtmMvcgrkd66LxKHtb9rKzcWdHaL+w54WO4LdZbJyRdQZTsqdQml5KaVopJZ4SStNKyXfnY7XIj+zJCMaCPLPrGZ7Y9gTesJfzSs/jtrm3MStvFm9Xvs0P1/6Qm/5xE5eVXcbX5n+NfHffD5wRqeuZXc/wo3U/YmzGWB76yEOMzxgPgNVi5QtzvsB5Y87j7nfu5guvf4Frpl7DXfPvSpmrSeQnXQw7p81I3hQl/ahtTf4I+4/42Ntghvb+ZK37rYoGIvHOZvIMp5WJ+WnJwPZQlpfGxHwP4Xj3UeVumxu3zU1pWukp/169sRm2jprylVwJQDwR51DboY7a9c6mnbxX/R4NwYaOZnEAq7JS6Ck0AzuthBJPiTlNM8O7wF3Qa3gndAJ/1E9rpJXWcKs5Pc58tCVKy54WFhYvpMhTNCTnxRvysvXIVoo8RUzOmjzo/fOhWIg/7/4zv9n6GxpDjSwpWcKtc29lTv6cjn2WjlnKOUXn8PjWx3li2xO8Xfk2t591O1dPvVr+KBphovEoP1r3I57d/SxLxyzlR+f/iHT70b9fZubO5JmPP8ODmx7k9zt/z5qaNdx3/n3dnn8wXJTWvV8Sc6pNnTpVV1RUDMtnj1Q9a4GjWTyhqfIGuzSR+9ifrHXXtIS67VuU4WR8rpsJuR7G5yWnuW7G53pIc6TuL91IPEKtv5YqXxXVvmpz6q/umG8IdA9vQxkUeYoodBcSjoc7wrYt0kZCJ475OYYyyLBnkOHIIN2WzgHvAXwJHwBlmWUsKl7EwuKFLChaQIY9Y1C+W42vho31G9lUt4mNdRvZ17KvY1u+K5/FJYtZUrKExSWLyXHmHOdIxxeOh/nL7r/w+NbHaQg2sLBoIbfOvZWzC88+7vsOth7kh2t/yLvV7zItZxorF63sFuQ9yc/mwJ2qc9YUauKu8rvYWLeRG2fdyB1n3dGv5wasrVnLytUraQg0cMvsW7h59s2D3sWklNqotZ7f954SziOK/ALon0Ak1hHUb27YhiWjkIONfg40BmhoC3fbNy/NwYRkUE/IdTM+z9OxnOlK7b7fY4V3nb8Op9VpBm4ydHudTy67re5uNdV/vvlPSueUsqZmDWtq1rCxbiPBWBCLsjArdxYLixeyuGQxc/Ln9Nm0D+Zguv0t+7uFcY2/BoA0WxpzC+Yyr3Aec/LnUNlWyerq1aypWUNLuAWFYnrudJaULGFJyRLm5s/t14CtaDzKC3te4LEtj1EXqGNe4Txum3sbC4oW9Pv8aq35x8F/cP/6+6kP1PPJyZ/kzrPv7PVRq/KzOXCn4pxVNFVwxz/voDHUyPeXfJ+PTfzYgN7fGmnlvrX38bd9f2NW7ix+eP4PKcssG7TySTifpuQXwMD1PGe+cIxDjYGOsD5wxM+BRj8HGwPUtnavcWe7bYzNcTM2282YbBdjctyMzXYxNsdNaZYLp+30fIpXz3MWjUfZ3LC5I6y3HdlGXMdxGk7mFc7rqFm3X/oWS8SoaKpgY91GNtZt5P369/GGvQDkOnM5u/Bs5hXO4+yCs5mSPaXXWk08EWdH4w7erX6Xd6vfZXPDZuI6jtvq5pyic1hSaob1uPRx3f6wiCaivLjnRX615VfU+GuYmz+X2866jYVFC0+4qdwf9fPLD37J73f+nnR7OneefSdXTr6y20h7+dkcuME+Z68dfI17Vt1Duj2dn1/wc2bmzTzhY/3jwD/4wZofEI6F+cq8r3DNtGsG5coKCefTlPwCGLiBnLNgJM6hpkAyrM3wrvQGqWwyp137uAEK0h3J8DYDe0y2i7HZbsbmuCnOdGI1RuZlUn2ds7ZIGxtqN7C2di1rqtewt2UvANmObMoyy9jVtItALADAmLQx3cJ4fMaJ3au9LdLGutp1vFv1LqurV1PlqwKgNK2UJSVLOLfkXFojrfxqy6+o8lUxO282t869lSUlSwat/3q3dzf/tea/2FS/idn5s/n2om8zLWcaMPJ+NoOxIDX+GqLxKGPTx+K2uYe8DIN1zhI6waObH+WXm3/J7PzZ/Gz5zwZlIF9DoIHvvPsdVlWtYnHxYu49996TvnGJhPNpaqT9AkgFg/YLIKGpbwtT6Q1w2BvgcFOQw03mfKU3SE1LiHiiS/+vRVGY7qAo00lxpis57T5fkO5IyQAf6DmrD9SztmYta2rWcKDlANNzp3eE8am6C9Oh1kO8W20G9bqadR1/DMzIncFtc2/j/NLzT8lNX7TWvLj3Rf5743/THG7m2mnXctvc29j47saU+tn0R/1U+6qp8ddQ5auixlfT0f1R7a8+6rr8PFce49LHMTZ9LOMyxpnzGZ2XBZ4Kg/GzGYgGuGfVPbx+6HUun3Q53178bRyGY3AKiPnv/efdf+YnG36C1WLtduMSrTWBWKDbcwS6TcNemkPdp29f87aE8+lIwnnghuqcReMJaltCHE7Wsg97A1Q3h6htNYO7pjlEMBrv9h6Lgvx0B0WZLooznBRnmaFdlOmiONNJaZZrWAJ8pP0/iyaibK43m73PKTpnSO7E1hJu4aH3H+LZimfJdeUyzZhGQXEB0XiUSCLSbRpNRInEI+Y0EemYb98W13FcVhdOw2lOrd2n7S+n4cRlc3Xb12E48Ia93QPYX01LuKVbee0WOyVpJRR7ijtG+JeklWC1WKlsq+RQ6yEOtR3icOth6oP13d6b7cg2LyHMGNstwEs8JcR1nHA8TCgWIhgLdsyH4iFz2mU+HA8TjAUJxUOEY2Ga65uZN2UexZ5iitOKKfYUk+fK6/fI+CpfFXf88w72NO/hq/O+ynUzrjtl//YHWw9y96q72dKwhQkZEzouyYwmor3ubyiDLEcW2c7sbtPvLvmuhPPpaKT90kwFqXLOtNa0BmPUJMO6tiVETXNyvjWUDPAg/kj3ADcsiqIMM6hLs12UZDkpzXJTkuVkTLZ5dza3fXBHnKfKORsJth/Zzo/X/5jdR3bjdrixG3ZsFhs2w4bdYj9q2Waxdaxrn1qUpSO42l/tYdc+bQ+1Y91Ex2V1dQZvl0vs2i+zy3Hm9LvPNBANUOlL3gWv9TCH2jqDu8Zf0+0KgYEwlNHxB4XT6sQb8BJIBI7ap9BdSJGnqCOwiz3F5nJyPs2exvra9Xy1/KvEdIyfLP3JkNxAJJaI8dSOp3i//v1uD/fpGr45zhyynFmk29J7/UNhIM3aqXsdiRCnEaUUmW4bmW4b04qO3UzYFopS0xKiujlIdXOIquZAchpk3f4malu7N58DZLltlCZvo9p+o5f2pvPCDPNlt6Ze8/npYGbeTJ5a8dSQ/UGT0InOGmiyZtoeEoNVa3Tb3EzJnsKU7ClHbYvEI1T5qjjcdpgaXw1WixWH1YHLcOGwOnAaTpxWZ+c0Oe+wOo66LKm8vJwF5y6g1l9Ljb/GfPlqOpY/qP+AV/2vEtOxbu9Lt6UTiAUYlzGu241FTjWrxcqNs24cks8CCWchUkq600a608aUwqNvmADm9d11rWZ4VyVf1c1BqrxBDjUGeG9vI75w7Kj35aXZKcp0UpTh7DJ1dS5nOlP6mm9hsihLRzP3cLAbdsoyywbt8iKPzcOkrElMyprU6/Z4Is6R4BFq/DXdQtxqsfKlOV/q9cYipwv5aRRiBDEsnQ8b6a1tTGtNayhGXbKpvLYlSG1LmNrWILUtISq9QTYe9OINHN1Xlu6wUpjpxB4P8te6DyhId5Cf7qAgwxy8VpCclxAXQ8WwGBR6ClP+8Y6ngvyUCXEaUUqR6bKR6Tp27RvM+5m3B3hnkJuvDysDrNvfRENb+KjLxwDcdiMZ1k7yMxwd82Z4OyjKcFKQ4STDaZXHZApxgiSchRiFnDaD8bkexud6jtrW3n+qtaYlGKW+LUx9a5j6ttBR8zurW3mrLdxrU7rLZlCU6aQwGdiFmU4K053Jdea0IN2BLQUvJxNiuEk4CyF6pZQiy20ny20/bi0cwB+O0dAWpq41RF1bmLrkKPTa1hB1LSE2HPRS33p0TVwpyPU4OgK8IMNJfntzenKan2ZOT9c7sgnRGwlnIcRJ8ziseBxWJuQdXRNvp7XGG4hSm2xKr201m9Hr28xpdUuIzZXNNPoj9HaFZ4bTaoZ3Wo/wbm9iT67LctukOV2MeBLOQoghoZQix2Mnx2NnRsmxLyeLxRM0+iM0tIVpaDOb0M1puGP6weFm6ttChKJH94nbDUu32ndBRvc+8fb53DQHhkVCXKQmCWchREqxGpaO67OPR2uNL9mcXt/+ag11WQ6x/4iftfubaAkePTrdoiA3zdExEj3bYyfbbSfbbSMzOc1228lKTrPddlx2aVoXQ0PCWQgxIimlOq4Ln5ifdtx9Q9F4l9p358C29pp5fVuY3XU+vIEIgR53aevKYbV0hHXn1I6vMcIhx4FkU7uzo8ld+snFiZJwFkKc9pw2w3yCWE7fT18Kx+K0BKJ4A1G8gQjNgQjNyeXmQARvIII3EKUlEGVPvQ9vIEqjL8r/7d1+1LEyXbaO5vT8tM5rxtv7yQsyHOSlOUh3WLFIE7voQsJZCCG6cFgNCjIMCvpoVu/qn2++yaz5i83auC9MQy+Xnm046KW+LUwkdnQ/uVKQ5rCS4bSR7rSS4bKR4ey+nN6xbCPDZTWnTiu5aQ65pvw0lFLhHI1GqaysJBQK9b3zKJSZmcnOnTuH7POcTidjxozBZrP1vbMQo5hFqeRAs777yVuDsc7gbgtxpC1CWyhKayhGayhKWyhGazBKVXOIXaE2WoNRfOEYieM8b8JhtXQOgEvWyM2aeveR7DIIbuRIqXCurKwkPT2dCRMmyF+BvWhrayM9fWjuJau1prGxkcrKSsrKBuc+ukKMdl0fgDK5j2vHu0okNP5IzAzuLgHeGopypC1Cg88cDFffFmZPg4/39jX2OQguP91BrseRrJFbu9XI051dp2aNXfrPh1ZKhXMoFJJgThFKKXJzc2loaBjuoggx6lksnYPfSujfQy+6D4LrMRDOZ9bad9e20RaK4YvEer22vCu7YekI6/Ygz3KZA+O6Do7Ldts6ptluOxkum9TWT0BKhTMgwZxC5N9CiJFrIIPgEgmNL1kzb+tSM29fbg1139YWitISjFLT0kpLIEpzMHrUo0zbKQUZTlu30M5y2/E1hdkc+7Aj3LPcdrJcyXmXnXTn6B4kl3LhPNzS0tLw+XzDXQwhhBgyFosiw2kjw2mDftbMu0okNG3hWHI0uznKvSU57Rzlbk4bfOZla02+GK8d3H3MYypljnbPcpnXnXcGt/lgl/a70qUlpx6H0THfPnXbjBEb8BLOQgghTorF0vk0tPG5/XtPeXk5556/lNagWfNuv2StOVkTbwlEkus7tx9o9NMcMPva+2qGBzPg3TbjqBDPdtvJ9tjJTd54pv3OdV1fw93HLuF8DFprvv71r/Pyyy+jlGLlypVcffXV1NTUcPXVV9Pa2kosFuOXv/wlS5Ys4aabbmLDhg0opbjxxhv5yle+MtxfQQghUprNsJCbZo4iH4hEQhOMxvGHY/jCMfzhOG3hKP5w13Wx5HxyXSS5HIrxYb2PJr95zfqxQt5lMzqCunuQtzfPdzbR53jMvvfBDPSUDefv/992dlS3DuoxZ5Rk8N1/mdmvfZ9//nk++OADNm/ezJEjR1iwYAFLly7l6aef5qMf/Sj33HMP8XicQCDABx98QFVVFdu2bQOgubl5UMsthBCik8WiOpq1C07iOPGEpjUYpTEZ1E3+zpe3fT5gzu9r8OH1R/Af5w5yLpvR2bfu6T4wLtttH1DZUjach9uqVau49tprMQyDwsJCli1bxvr161mwYAE33ngj0WiUK664grlz5zJx4kT27dvHl7/8ZT72sY9xySWXDHfxhRBC9MGwKPOe6p7+B2c4Fk/eMS6C1282tze130XOH+l2J7ma5lbzLnPB/jXDd5Wy4dzfGu6poo9xJpcuXcrbb7/N3//+d6677jr+4z/+g89+9rNs3ryZV199lYcffphnn32WJ554YohLLIQQ4lRzWA0KM4w+H8zSVSKhaQ1Fyf5x/z/HcgJlGxWWLl3KM888Qzwep6GhgbfffptzzjmHgwcPUlBQwM0338xNN93Epk2bOHLkCIlEgk9+8pPce++9bNq0abiLL4QQIkVYLIosadYeHFdeeSXvvfcec+bMQSnF/fffT1FREb/73e944IEHsNlspKWl8dRTT1FVVcXnPvc5Egnznrn33XffMJdeCCHESNavcFZKXQo8CBjA41rrH/XYfhfweSAGNAA3aq0PDnJZh0T7Nc5KKR544AEeeOCBbtuvv/56rr/++qPeJ7VlIYQQg6XPZm2llAE8DKwAZgDXKqVm9NjtfWC+1no28Bxw/2AXVAghhBgt+tPnfA6wR2u9T2sdAf4EXN51B631m1rrQHJxDTBmcIsphBBCjB79adYuBQ53Wa4EFh5n/5uAl3vboJS6BbgFID8/n/Ly8m7bMzMzaWtr60eRRqd4PD7k5ycUCh317zSS+Hy+EV3+4SDnbODknA2cnLPj608493Zj0l6vM1JK/TswH1jW23at9WPAYwBTp07Vy5cv77Z9586dQ/ZIxJFoKB8Z2c7pdHLWWWcN6WcOpvLycnr+PxPHJ+ds4OScDZycs+PrTzhXAmO7LI8BqnvupJS6CLgHWKa1Dg9O8YQQQojRpz99zuuByUqpMqWUHbgGeLHrDkqps4BfAZ/QWtcPfjGFEEKI0aPPcNZax4DbgVeBncCzWuvtSqkfKKU+kdztASAN+LNS6gOl1IvHOJwQQggh+tCv65y11i8BL/VY950u8xcNcrlOe7FYDKtV7gEjhBDiaHL7zl5cccUVzJs3j5kzZ/LYY48B8Morr3D22WczZ84cLrzwQsAcbfi5z32OM888k9mzZ/OXv/wFgLS0tI5jPffcc9xwww0A3HDDDdx1111ccMEFfOMb32DdunUsWbKEs846iyVLllBRUQGYo7K/9rWvdRz3oYce4o033uAzn/lMx3Ffe+01rrrqqqE4HUIIIe/TUs4AAA5rSURBVIZY6lbdXv4m1G4d3GMWnQkrftTnbk888QQ5OTkEg0EWLFjA5Zdfzs0338zbb79NWVkZTU1NANx7771kZmaydatZTq/X2+exd+/ezeuvv45hGLS2tvL2229jtVp5/fXXufvuu/nLX/7CY489xv79+3n//fexWq00NTWRnZ3Nl770JRoaGsjPz+fJJ5/kc5/73MmdDyGEECkpdcN5GP385z/nhRdeAODw4cM89thjLF26lLKyMgBycnIAeP311/nTn/7U8b7s7Ow+j/3pT38awzAfyN3S0sL111/Phx9+iFL/f3v3GxxVleZx/PsALQFEDKIhAQSdNQQhiazUgLJAgNrATkXYpQIEwZqlxFlkBxRKZYOAKQUKKcHRwgLRGTESFihcdiml2F2KBFYKXMIUZcBg1kLUCPInBCQvQv6dfZGmJyTp0A0JfUN+nzfpvn373qefnMqTc27fc4zKysrAcWfPnh0Y9r52voyMDDZt2sTMmTM5ePAg2dnZzfSJRUTES7xbnEPo4baEvLw89uzZw8GDB+ncuTMpKSkkJycHhpzrcs5h1vA28LrbysvLr3utS5cugcdLlixh9OjR7Nixg1OnTgXu+Qt23BkzZjBt2jSioqKYPHmyrlmLiNyhdM25nsuXLxMdHU3nzp05ceIEhw4d4urVq+zbt4/vvvsOIDCsnZqaytq1awPvvTasHRMTQ2FhITU1NYEeeLBz9erVC4CNGzcGtqemprJ+/XqqqqquO19sbCxxcXEsW7YscB1bRETuPCrO9YwfP56qqiqSkpJYsmQJw4YN4/7772fDhg1MmjSJ5ORkpk6dCsDixYspLS1l0KBBJCcnk5ubC8DKlStJS0tjzJgxxMbGBj3XK6+8QmZmJsOHD6e6ujqwfdasWTz44IMkJSWRnJzM5s2bA69Nnz6dPn368Oij9dceERGRO4U51+hMnC2uf//+rv5QcWFhIQMGDIhIPK3BlStXyMzMZPDgwTz77LO35Zyt/XeiKQLDp5yFTzkLX1vMmZkdcc4NCWVfXbRsRUaOHEnXrl1ZvXp1pEMREZEWpOLciuzfv18Lg4iItAG65iwiIuIxKs4iIiIeo+IsIiLiMSrOIiIiHqPiLCIi4jEqzreg7upT9Z06dYpBgwbdxmhEROROoeIsIiLiMZ69z/nN/32TExdPNOsxE7onsPDXC4O+vnDhQvr27cucOXMAyMrKwszYv38/paWlVFZWsmzZMiZOnBjWecvLy3n++efJz8+nQ4cOrFmzhtGjR3P8+HFmzpxJRUUFNTU1fPrpp8TFxTFlyhSKi4uprq5myZIlgelCRUSkbfBscY6EjIwMXnzxxUBx3rZtG7t372b+/Pncc889XLhwgWHDhjFhwoRGV40K5r333gOgoKCAEydOkJqaSlFREevXr+eFF15g+vTpVFRUUF1dza5du4iLi+Pzzz8HahfHEBGRtsWzxbmpHm5LGTx4MOfOneP06dOcP3+e6OhoYmNjmT9/Pvv376ddu3b89NNPnD17lp49e4Z83C+++IK5c+cCkJCQQN++fSkqKuKJJ55g+fLlFBcXM2nSJB555BESExN56aWXWLhwIWlpaYwYMaKlPq6IiHiUrjnXk56ezvbt29m6dSsZGRnk5ORw/vx5jhw5wtGjR4mJiWmwRvONBFtc5Omnn2bnzp106tSJcePGsXfvXuLj4zly5AiJiYlkZmby+uuvN8fHEhGRVsSzPedIycjI4LnnnuPChQvs27ePbdu28cADD+Dz+cjNzeX7778P+5gjR44kJyeHMWPGUFRUxA8//ED//v05efIkDz/8MPPmzePkyZN89dVXJCQk0L17d2bMmMHdd9993TrPIiLSNqg41zNw4ECuXLlCr169iI2NZfr06Tz11FMMGTKExx57jISEhLCPOWfOHGbPnk1iYiIdOnRg48aNdOzYka1bt7Jp0yZ8Ph89e/Zk6dKlHD58mJdffpl27drh8/lYt25dC3xKERHxMhXnRhQUFAQe9+jRg4MHDza6X1lZWdBj9OvXj2PHjgEQFRXVaA84MzOTzMzM67aNGzeOcePG3UTUIiJyp9A1ZxEREY9Rz/kWFRQU8Mwzz1y3rWPHjnz55ZcRikhERFo7FedblJiYyNGjRyMdhoiI3EE0rC0iIuIxKs4iIiIeo+IsIiLiMSrOIiIiHqPifAuaWs9ZRETkZqk43wGqqqoiHYKIiDQjz95K9fOKFVwtbN71nDsOSKDnokVBX2/O9ZzLysqYOHFio+/Lzs7mrbfewsxISkrik08+4ezZs8yePZuTJ08CsG7dOuLi4khLSwvMNPbuu+9SWVlJVlYWKSkpPPnkkxw4cIAJEyYQHx/PsmXLqKio4L777iMnJ4eYmBjKysqYO3cu+fn5mBmvvfYaly5d4tixY7z99tsAfPDBBxQWFrJmzZpbyq+IiDQPzxbnSGjO9ZyjoqLYsWNHg/d9/fXXLF++nAMHDtCjRw8uXrwIwLx58xg1ahQ7duygurqasrIySktLmzzHpUuX2LdvHwClpaUcOnQIM+PDDz9k1apVrF69mjfeeINu3boFpiQtLS3lrrvuIikpiVWrVuHz+fjoo494//33bzV9IiLSTDxbnJvq4baU5lzP2TnHokWLGrxv7969pKen06NHDwC6d+8OwN69e8nOzgagffv2dOvW7YbFeerUqYHHxcXFTJ06lTNnzlBRUcFDDz0EwJ49e9iyZUtgv+joaADGjBnDZ599xoABA6isrCQxMTHMbImISEvxbHGOlGvrOf/8888N1nP2+Xz069cvpPWcg73POXfDXvc1HTp0oKamJvC8vLyc9u3bB5536dIl8Hju3LksWLCACRMmkJeXR1ZWFkDQ882aNYsVK1aQkJDAzJkzQ4pHRERuD30hrJ6MjAy2bNnC9u3bSU9P5/Llyze1nnOw940dO5Zt27ZRUlICEBjWHjt2bGB5yOrqan755RdiYmI4d+4cJSUlXL16ld27dzd5vl69egHw8ccfB7anpqaydu3awPNrvfGhQ4fy448/snnzZqZNmxZqekRE5DZQca6nsfWc8/PzGTJkCDk5OSGv5xzsfQMHDuTVV19l1KhRJCcns2DBAgDeeecdcnNzSUxM5PHHH+f48eP4fD6WLl3K0KFDSUtLIz4+Puj5srKymDx5MiNGjAgMmQMsXryY0tJSBg0aRHJyMrm5uYHXpkyZwvDhwwND3SIi4g3mnIvIifv37++++eab67YVFhYyYMCAiMTTGly5coWuXbs22/HS0tKYP38+Y8eODbpPa/+d5OXlkZKSEukwWhXlLHzKWfjaYs7M7Ihzbkgo+6rn3AZdunSJ+Ph4OnXq1GRhFhGRyNAXwm5Ra1zP+d5776WoqCjSYYiISBAqzrdI6zmLiEhz89ywdqSugUtD+l2IiESGp4pzVFQUJSUlKgoe4JyjpKSEqKioSIciItLmeGpYu3fv3hQXF3P+/PlIh+JJ5eXlt7VYRkVF0bt379t2PhERqRVScTaz8cA7QHvgQ+fcynqvdwSygceBEmCqc+5UuMH4fL7AtJPSUF5eHoMHD450GCIi0sJuOKxtZu2B94C/Ax4FppnZo/V2exYodc79FfA28GZzByoiItJWhHLN+dfAt865k865CmALUH/NxInAtTkjtwNjLdQJpEVEROQ6oRTnXsCPdZ4X+7c1uo9zrgq4DNzXHAGKiIi0NaFcc26sB1z/69Sh7IOZ/Q74nf/pVTM7FsL55S96ABciHUQro5yFTzkLn3IWvraYs76h7hhKcS4G+tR53hs4HWSfYjPrAHQDLtY/kHNuA7ABwMzyQ51jVGopZ+FTzsKnnIVPOQufcta0UIa1DwOPmNlDZnYXkAHsrLfPTuC3/sfpwF6nm5VFRERuyg17zs65KjP7PfCf1N5K9Sfn3HEzex3Id87tBP4IfGJm31LbY85oyaBFRETuZCHd5+yc2wXsqrdtaZ3H5cDkMM+9Icz9RTm7GcpZ+JSz8Cln4VPOmhCx9ZxFRESkcZ6aW1tEREQiVJzNbLyZfWNm35rZv0QihtbGzE6ZWYGZHTWz/EjH40Vm9iczO1f3Fj0z625m/21m/+f/GR3JGL0mSM6yzOwnf1s7ama/iWSMXmJmfcws18wKzey4mb3g3652FkQTOVM7a8JtH9b2TwdaBPwttbdgHQamOee+vq2BtDJmdgoY4pxra/cFhszMRgJlQLZzbpB/2yrgonNupf8fwWjn3MJIxuklQXKWBZQ5596KZGxeZGaxQKxz7s9m1hU4Avw98I+onTWqiZxNQe0sqEj0nEOZDlQkbM65/TS8v77u1LIfU/tHQfyC5EyCcM6dcc792f/4ClBI7QyJamdBNJEzaUIkinMo04FKQw74LzM74p9pTUIT45w7A7V/JIAHIhxPa/F7M/vKP+ytIdpGmFk/YDDwJWpnIamXM1A7CyoSxTmkqT6lgeHOub+mdnWwf/YPR4q0hHXAr4DHgDPA6siG4z1mdjfwKfCic+6XSMfTGjSSM7WzJkSiOIcyHajU45w77f95DthB7eUBubGz/mte1659nYtwPJ7nnDvrnKt2ztUAH6C2dh0z81FbZHKcc//m36x21oTGcqZ21rRIFOdQpgOVOsysi/+LFJhZFyAV0KIhoak7texvgf+IYCytwrUi4/cPqK0F+JfC/SNQ6JxbU+cltbMgguVM7axpEZmExP+V+T/wl+lAl9/2IFoRM3uY2t4y1M7qtlk5a8jM/hVIoXa1m7PAa8C/A9uAB4EfgMnOOX0Byi9IzlKoHWp0wCngn65dT23rzOxvgP8BCoAa/+ZF1F5DVTtrRBM5m4baWVCaIUxERMRjNEOYiIiIx6g4i4iIeIyKs4iIiMeoOIuIiHiMirOIiIjHqDiLiIh4jIqziIiIx6g4i4iIeMz/A6WADUlzIsprAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = hist_obj\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5)) \n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/sample - loss: 66.8660 - accuracy: 0.8416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[66.86598207092285, 0.8416]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only care about the class with the highest estima‐ ted probability (even if that probability is quite low) then you can use the pre `dict_classes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target);\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full);\n",
    "scaler = StandardScaler();\n",
    "X_train_scaled = scaler.fit_transform(X_train);\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled =  scaler.transform(X_valid);\n",
    "X_test_scaled =  scaler.transform(X_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With respect to the book, batch_size has been set because otherwise the values were None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.7743    ,   20.        ,    3.96111975, ...,    1.86469673,\n",
       "          32.76      , -117.08      ],\n",
       "       [   1.6354    ,   37.        ,    2.70588235, ...,    3.73333333,\n",
       "          34.01      , -118.16      ],\n",
       "       [   5.145     ,   19.        ,    4.21603261, ...,    1.66779891,\n",
       "          33.83      , -118.43      ],\n",
       "       ...,\n",
       "       [   5.0853    ,   26.        ,    5.91649695, ...,    2.95723014,\n",
       "          33.68      , -117.89      ],\n",
       "       [   4.4545    ,   24.        ,    4.49760766, ...,    2.31339713,\n",
       "          33.76      , -117.98      ],\n",
       "       [   3.6815    ,   17.        ,    4.76586433, ...,    2.35667396,\n",
       "          33.82      , -118.03      ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 26us/sample - loss: 1.3898 - val_loss: 1.2725\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 9us/sample - loss: 0.6372 - val_loss: 0.6048\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 9us/sample - loss: 0.5582 - val_loss: 0.5565\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 9us/sample - loss: 0.5238 - val_loss: 0.5434\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.5030 - val_loss: 0.5082\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 9us/sample - loss: 0.4869 - val_loss: 0.5087\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4785 - val_loss: 0.4904\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 9us/sample - loss: 0.4693 - val_loss: 0.4840\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4604 - val_loss: 0.4685\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4564 - val_loss: 0.4659\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4514 - val_loss: 0.4618\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4467 - val_loss: 0.4652\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4427 - val_loss: 0.4464\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4350 - val_loss: 0.4458\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4305 - val_loss: 0.4408\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4272 - val_loss: 0.4513\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4227 - val_loss: 0.4335\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4196 - val_loss: 0.4371\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4167 - val_loss: 0.4299\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 8us/sample - loss: 0.4161 - val_loss: 0.4348\n",
      "5160/5160 [==============================] - 0s 17us/sample - loss: 0.4093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.5967293],\n",
       "       [2.7552896],\n",
       "       [2.7130415]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "]);\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd');\n",
    "history = model.fit(X_train_scaled, y_train, batch_size=128, epochs=20, validation_data=(X_valid_scaled, y_valid));\n",
    "mse_test= model.evaluate(X_test_scaled, y_test);\n",
    "X_new = X_test_scaled[:3];\n",
    "y_pred = model.predict(X_new);\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5967293],\n",
       "       [2.7552896],\n",
       "       [2.7130415]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 1509460636897077424226828288.0000 - val_loss: 74537617281167392768.0000\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 5187975148315064320.0000 - val_loss: 31805786678869.4648\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 2213749890607.1548 - val_loss: 13571862.3318\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 944632.6265 - val_loss: 7.2281\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.7574 - val_loss: 1.3197\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3468 - val_loss: 1.3198\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3197\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3467 - val_loss: 1.3197\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3199\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3467 - val_loss: 1.3197\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3465 - val_loss: 1.3203\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3200\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3465 - val_loss: 1.3209\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3198\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3466 - val_loss: 1.3196\n",
      "5160/5160 [==============================] - 0s 17us/sample - loss: 398.7690\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                        validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0775273],\n",
       "       [2.0775273],\n",
       "       [2.0775273]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Neural network \n",
    "It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers, thus simple patterns in the data may end up being distorted by this sequence of transfor‐ mations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=X_train.shape[1:])\n",
    "#notice that we call hidden1 like a function, passing it the input\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input, hidden2])\n",
    "output = keras.layers.Dense(1)(concat);\n",
    "\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you want to send a subset of the features through the wide path, and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = keras.layers.Input(shape=[5]);\n",
    "input_b = keras.layers.Input(shape=[6]);\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_b)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_a, hidden2])\n",
    "output = keras.layers.Dense(1)(concat);\n",
    "\n",
    "model = keras.models.Model(inputs=[input_a, input_b], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: nan - val_loss: nan\n",
      "5160/5160 [==============================] - 0s 18us/sample - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "X_train_a, X_train_b = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_a, X_valid_b = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_a, X_test_b = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_a = X_test_a[:3]\n",
    "X_new_b = X_test_b[:3]\n",
    "history = model.fit((X_train_a, X_train_b), y_train, batch_size=512, epochs=20, \n",
    "                    validation_data=((X_valid_a, X_valid_b), y_valid));\n",
    "mse_test = model.evaluate((X_test_a, X_test_b), y_test);\n",
    "y_pred = model.predict((X_new_a, X_new_b))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra output\n",
    "Each output will need its own loss function, so when we compile the model we should pass a list of losses (if we pass a single loss, Keras will assume that the same loss must be used for all outputs).\n",
    "\n",
    "y default, Keras will compute all these losses and simply add them up to get the final loss used for training. However, we care much more about the main output than about the auxiliary output (as it is just used for reg‐ ularization), so we want to give the main output’s loss a much greater weight.\n",
    "\n",
    "Fortunately, it is possible to set all the loss weights when compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1)(concat);\n",
    "aux_output = keras.layers.Dense(1)(hidden2);\n",
    "model = keras.models.Model(inputs=[input_a, input_b], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train the model, we need to provide some labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we just need to pass (y_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan - val_loss: nan - val_dense_16_loss: nan - val_dense_17_loss: nan\n",
      "5160/5160 [==============================] - 0s 21us/sample - loss: nan - dense_16_loss: nan - dense_17_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_a, X_train_b], [y_train, y_train], epochs=20,\n",
    "                   validation_data=((X_valid_a, X_valid_b), (y_valid, y_valid)));\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_a, X_test_b), (y_test, y_test));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_a, X_new_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name) \n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation) \n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation) \n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2]) \n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` method accepts a callbacks argument that lets you specify a list of objects that Keras will call during training at the start and end of training, at the start and end of each epoch and even before and after processing each batch. For example, the `ModelCheckpoint` callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch. This is a simple way to implement early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: nan\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "]);\n",
    "#model.compile(loss='mean_squared_error', optimizer='sgd');\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if you use a validation set during training, you can set `save_best_only=True` when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 20996345791750814212030464.0000 - val_loss: 63369817090357823799296.0000\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 4410673573105780654080.0000 - val_loss: 27040396836077568.0000\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1882068518016501.2500 - val_loss: 11538376483.8532\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 803095258.2085 - val_loss: 4924.4671\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 344.0041 - val_loss: 1.3229\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3197\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3467 - val_loss: 1.3197\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3466 - val_loss: 1.3199\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "]);\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "                                                    save_best_only=True)\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 17us/sample - loss: 511870802461896736768.0000 - val_loss: 2103295237752356864.0000\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1405150825597026560.0000 - val_loss: 830424390552193152.0000\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 554781930568240576.0000 - val_loss: 327867986648454016.0000\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 219039083009756896.0000 - val_loss: 129448839371936992.0000\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 86481018454904848.0000 - val_loss: 51109055698759816.0000\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 34144455802492472.0000 - val_loss: 20178875354291532.0000\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 13480919010491390.0000 - val_loss: 7967027431997440.0000\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 5322538986874942.0000 - val_loss: 3145547591535702.5000\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 2101447967281656.7500 - val_loss: 1241924729923411.5000\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 829694249066096.7500 - val_loss: 490337311435231.4375\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 327580204515251.7500 - val_loss: 193595271849515.1250\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 129335391822686.2344 - val_loss: 76435379804154.7031\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 51064233813899.9297 - val_loss: 30178200539654.6133\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 20161184144892.9141 - val_loss: 11914971723800.3438\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 7960044730366.9414 - val_loss: 4704269577907.6631\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 3142783939650.1499 - val_loss: 1857339656176.1240\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1240834783482.1348 - val_loss: 733315353516.9158\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 489906471933.7068 - val_loss: 289527812394.4682\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 193425159019.6479 - val_loss: 114311512826.0465\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 76368183081.4980 - val_loss: 45132476684.8331\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 30151685079.9573 - val_loss: 17819204714.3690\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 11904491065.2417 - val_loss: 7035382771.2992\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 4700134463.7244 - val_loss: 2777712284.9075\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1855708682.6501 - val_loss: 1096697583.2641\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 732671673.7433 - val_loss: 432998581.3499\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 289273710.3876 - val_loss: 170956677.1101\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 114211131.3998 - val_loss: 67497171.2248\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 45092874.8520 - val_loss: 26649249.5638\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 17803577.3256 - val_loss: 10521662.2000\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 7029209.2606 - val_loss: 4154165.4736\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 2775274.7643 - val_loss: 1640150.7077\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1095734.7876 - val_loss: 647566.3112\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 432620.0827 - val_loss: 255674.9723\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 170808.7587 - val_loss: 100947.1990\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 67439.9486 - val_loss: 39857.2418\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 26627.6181 - val_loss: 15737.5620\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 10514.0305 - val_loss: 6214.3533\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 4151.8942 - val_loss: 2454.3683\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1640.1033 - val_loss: 969.9774\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 648.4358 - val_loss: 383.7921\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 256.8273 - val_loss: 152.3314\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 102.2116 - val_loss: 60.9432\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 41.1668 - val_loss: 24.8716\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 17.0717 - val_loss: 10.6176\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 7.5514 - val_loss: 4.9921\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 3.7939 - val_loss: 2.7668\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 2.3107 - val_loss: 1.8900\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.7260 - val_loss: 1.5452\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.4960 - val_loss: 1.4081\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.4047 - val_loss: 1.3544\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3692 - val_loss: 1.3334\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3552 - val_loss: 1.3251\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3498 - val_loss: 1.3218\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3477 - val_loss: 1.3205\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3468 - val_loss: 1.3200\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3465 - val_loss: 1.3198\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3197\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 0s 2us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 0s 2us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 0s 2us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3462 - val_loss: 1.3196\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 3us/sample - loss: 1.3463 - val_loss: 1.3196\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                      restore_best_weights=True)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "]);\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=100,batch_size=512,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, you can implement `on_train_begin()`, `on_train_end()`, `on_epoch_begin()`, `on_epoch_begin()`, `on_batch_end()` and `on_batch_end()`.\n",
    "\n",
    "Moreover, callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). In this case, you should implement `on_test_begin()`, `on_test_end()`, `on_test_batch_begin()`, or on_test_batch_end() (called by evaluate()), or on_predict_begin(), on_pre dict_end(), `on_predict_batch_begin()`, or `on_predict_batch_end()` (called by `predict()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Using TensorBoard\n",
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, vis‐ ualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! \n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visu‐ alize to special binary log files called event files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\");\n",
    "def get_run_logdir(): \n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\");\n",
    "    return os.path.join(root_logdir, run_id);\n",
    "\n",
    "run_logdir = get_run_logdir();\n",
    "\n",
    "#tensorboard callback\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (n_hidden=1, n_neurons=3, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential();\n",
    "    options = {\"input_shape\":input_shape};\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options));\n",
    "        options={};\n",
    "    model.add(keras.layers.Dense(1, **options));\n",
    "    optimizer = keras.optimizers.SGD(learning_rate);\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer);\n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's crete a `KerasRegressor` for the defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 962193.4992 - val_loss: 88.9526\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 21.2950 - val_loss: 2.4401\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 1.6016 - val_loss: 1.3348\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.3497 - val_loss: 1.3197\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 1.3462 - val_loss: 1.3199\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 1.3465 - val_loss: 1.3197\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.3465 - val_loss: 1.3196\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "5160/5160 [==============================] - 0s 18us/sample - loss: 1.3075\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model);\n",
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)]);\n",
    "mse_test = keras_reg.score(X_test, y_test);\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 45us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 42us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 45us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 321006748755000.0000 - val_loss: 192473718068.5230\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 66670530821.9535 - val_loss: 12789072718.7183\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 4429977572.2171 - val_loss: 849779033.1370\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 294353325.9886 - val_loss: 56464298.3938\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 19558574.5532 - val_loss: 3751800.3388\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1299580.6153 - val_loss: 249284.9467\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 86350.8010 - val_loss: 16563.4854\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 5739.0151 - val_loss: 1101.7778\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 382.6776 - val_loss: 74.3935\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 26.6727 - val_loss: 6.1488\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 3.0240 - val_loss: 1.6351\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.4579 - val_loss: 1.3399\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3546 - val_loss: 1.3209\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3472 - val_loss: 1.3196\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3467 - val_loss: 1.3198\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3467 - val_loss: 1.3197\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3467 - val_loss: 1.3196\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3468 - val_loss: 1.3196\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3468 - val_loss: 1.3197\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: 1.3456\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: 1.3466\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 9500850153408192512.0000 - val_loss: 3919520043384705.0000\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1357674171746093.2500 - val_loss: 260435980135167.3438\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 90211852141991.8906 - val_loss: 17304895762404.4824\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 5994206024638.9082 - val_loss: 1149839528742.4993\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 398290512165.1762 - val_loss: 76402185692.8083\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 26464775322.7907 - val_loss: 5076605802.2367\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1758473666.4972 - val_loss: 337319249.2982\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 116843254.2760 - val_loss: 22413518.0289\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 7763745.1535 - val_loss: 1489291.5755\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 515868.6435 - val_loss: 98961.4836\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 34277.9718 - val_loss: 6576.9428\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 2279.1036 - val_loss: 438.6839\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 152.7033 - val_loss: 30.4047\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 11.3940 - val_loss: 3.2653\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 2.0071 - val_loss: 1.4523\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3821 - val_loss: 1.3295\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3408 - val_loss: 1.3212\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3378 - val_loss: 1.3199\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3374 - val_loss: 1.3197\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3374 - val_loss: 1.3196\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3374 - val_loss: 1.3197\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3374 - val_loss: 1.3197\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3197\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3374 - val_loss: 1.3197\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3372 - val_loss: 1.3196\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3374 - val_loss: 1.3196\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3196\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3373 - val_loss: 1.3197\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: 1.3645\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: 1.3372\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1763259730983116133629952.0000 - val_loss: 772970888812851888128.0000\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: 267747742700115230720.0000 - val_loss: 51360739000393400320.0000\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 17790732802517827584.0000 - val_loss: 3412710092770771968.0000\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1182121393013761536.0000 - val_loss: 226760580739883072.0000\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 78547116626006368.0000 - val_loss: 15067318644178436.0000\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 5219134353340381.0000 - val_loss: 1001160632609044.2500\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 346789746225683.3125 - val_loss: 66522986012041.1953\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 23042748097007.3281 - val_loss: 4420177942573.5107\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1531095532629.2012 - val_loss: 293702806694.6977\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 101735086915.0760 - val_loss: 19515343706.3607\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 6759875514.8734 - val_loss: 1296714821.5235\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 449165668.6181 - val_loss: 86161180.3245\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 29845117.1656 - val_loss: 5725092.3881\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1983094.9831 - val_loss: 380421.4854\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 131771.1744 - val_loss: 25280.4858\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 8756.5070 - val_loss: 1680.8280\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 582.9156 - val_loss: 113.0460\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 40.0049 - val_loss: 8.7685\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 3.9151 - val_loss: 1.8197\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.5254 - val_loss: 1.3563\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3674 - val_loss: 1.3237\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3562 - val_loss: 1.3203\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3551 - val_loss: 1.3198\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3197\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3196\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3198\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 37us/sample - loss: 1.3549 - val_loss: 1.3197\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3196\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3550 - val_loss: 1.3196\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3196\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3549 - val_loss: 1.3196\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3198\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3196\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3550 - val_loss: 1.3198\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3549 - val_loss: 1.3198\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3196\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3197\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: 1.3550 - val_loss: 1.3196\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.3549 - val_loss: 1.3197\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: 1.3295\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: 1.3548\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 18us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 72us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 103us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 16us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 17us/sample - loss: nan\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "   32/11610 [..............................] - ETA: 50s - loss: 4555.6953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fra/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610 [==============================] - 1s 47us/sample - loss: 86799359364804032.0000 - val_loss: 19064074739885.5781\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 4635240729717.3057 - val_loss: 326525904486.4000\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 79391500544.3969 - val_loss: 5592676913.2155\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1359803320.3294 - val_loss: 95790303.0078\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 23290464.6139 - val_loss: 1640691.3371\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 398916.8877 - val_loss: 28102.6829\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 6834.0673 - val_loss: 482.9552\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 118.4501 - val_loss: 9.5636\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 3.3485 - val_loss: 1.4663\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3822 - val_loss: 1.3225\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3469 - val_loss: 1.3196\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3463 - val_loss: 1.3199\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 1.3463 - val_loss: 1.3198\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3463 - val_loss: 1.3196\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3463 - val_loss: 1.3197\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3463 - val_loss: 1.3197\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3463 - val_loss: 1.3197\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3463 - val_loss: 1.3197\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3197\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3196\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 1.3465 - val_loss: 1.3196\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 1.3464 - val_loss: 1.3196\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_distribs = {\n",
    "    \"n_hidden\"  : [0,1,2,3],\n",
    "    \"n_neurons\" : np.arange(1,100),\n",
    "    \"learning_rate\" : reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3);\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.0027931685350825347, 'n_hidden': 1, 'n_neurons': 19},\n",
       " -1.3465237868848696)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_, rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when training is slow (e.g., for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space. You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranges of values centered on the best ones found during the first run, and so on. This will hopefully zoom in to a good set of hyperparameters. However, this is very time consuming, and probably not the best use of your time.\n",
    "\n",
    "Fortunately, there are many techniques to explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more.\n",
    "\n",
    "## Here are a few guidelines for choosing the number of hidden layers and neurons in an MLP, and selecting good values for some of the main hyperparameters.\n",
    "\n",
    "### Number of hidden layers\n",
    "For many problems, you can just begin with a single hidden layer and you will get reasonable results. Despite this, deep NN can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.\n",
    "\n",
    "Lower hidden layers model low-level struc‐ tures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures.\n",
    "\n",
    "Not only does this hierarchical architecture help DNNs converge faster to a good sol‐ ution, it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures, and you now want to train a new neural network to recognize hairstyles, then you can kickstart training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the value of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called ***transfer learning***.\n",
    "\n",
    "Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers.\n",
    "\n",
    "### Number of Neurons per hidden layer\n",
    "Obviously the number of neurons in the input and output layers is determined by the type of input and output your task requires. \n",
    "\n",
    "As for the hidden layers, it used to be a common practice to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low- level features can coalesce into far fewer high-level features. \n",
    "\n",
    "However, this practice has been largely abandoned now, as it seems that simply using the same number of neurons in all hid‐ den layers performs just as well in most cases, or even better, and there is just one hyperparameter to tune instead of one per layer—for example, all hidden layers could simply have 150 neurons. However, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "\n",
    "Just like for the number of layers, you can try increasing the number of neurons grad‐ ually until the network starts overfitting. In general you will get more bang for the buck by increasing the number of layers than the number of neurons per layer. Unfortunately, as you can see, finding the perfect amount of neurons is still somewhat of a dark art.\n",
    "\n",
    "A simpler approach is to pick a model with more layers and neurons than you actually need, then use early stopping to prevent it from overfitting.\n",
    "\n",
    "### Learning Rate, Batch Size and Other Hyperparameters\n",
    "- The **learning rate** is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). So a simple approach for tuning the learning rate is to start with a large value that makes the training algorithm diverge, then divide this value by 3 and try again, and repeat until the training algorithm stops diverging. At that point, you generally won’t be too far from the optimal learning rate. That said, it is sometimes useful to reduce the learning rate during training\n",
    "- Choosing a better **optimizer** than plain old Mini-batch Gradient Descent \n",
    "- In general the **optimal batch size** will be lower than 32. A small batch size ensures that each training iteration is very fast, and although a large batch size will give a more precise estimate of the gradi‐ ents, in practice this does not matter much since the optimization landscape is quite complex and the direction of the true gradients do not point precisely in the direction of the optimum. However, having a batch size greater than 10 helps take advantage of hardware and software optimizations, in particular for matrix multiplications, so it will speed up training. Moreover, if you use Batch Normal‐ ization (see Chapter 11), the batch size should not be too small (in general no less than 20).\n",
    "- in general, the ReLU **activation function** will be a good default for all hidden layers. For the output layer, it really depends on your task.\n",
    "- In most cases, the **number of training iterations** does not actually need to be tweaked: just use early stopping instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(2, activation='relu', input_shape=[2]),\n",
    "    keras.layers.Dense(2, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='relu')\n",
    "]);\n",
    "model.compile(optimizer='sgd', loss=\"mse\")\n",
    "X_train = [[0,0], [0,1],[1,0],[1,1]];\n",
    "y_train = [0,1,1,0];\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "A classical Perceptron will converge only if the dataset is linearly separable, and it won’t be able to estimate class probabilities. In contrast, a Logistic Regression classifier will converge to a good solution even if the dataset is not linearly separable, and it will output class probabilities. If you change the Perceptron’s activation function to the logistic activation function (or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent (or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The logistic activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Name three popular activation functions. Can you draw them?\n",
    "sigmoid/softmax, arctan, relu, step funcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "## - What is the shape of the input matrix $X$?\n",
    "## - What about the shape of the hidden layer’s weight vector $W_h$, and the shape of its bias vector $b_h$?\n",
    "## - What is the shape of the output layer’s weight vector $W_o$, and its bias vector $b_o$?\n",
    "## - What is the shape of the network’s output matrix $Y$?\n",
    "## - Write the equation that computes the network’s output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$ and $b_o$.\n",
    "\n",
    "- The shape of the input matrix $X$ is $m × 10$, where m represents the training batch size.\n",
    "- $W_h$: $10\\times 50$, $b_h$: $50$\n",
    "- $W_o$: $50\\times 3$, $b_o$: $3$\n",
    "- $m\\times 3$\n",
    "- $$Y = sigmoid(sigmoid(X W_h + b_h) W_o+b_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "\n",
    "To classify email into spam or ham, you just need one neuron in the output layer of a neural network—for example, indicating the probability that the email is spam. You would typically use the logistic activation function in the output layer when estimating a probability. If instead you want to tackle MNIST, you need 10 neurons in the output layer, and you must replace the logistic function with the softmax activation function, which can handle multiple classes, outputting one probability per class. Now, if you want your neural network to predict housing prices like in Chapter 2, then you need one output neuron, using no activation function at all in the output layer.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) What is backpropagation?\n",
    "Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame‐ ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou‐ sands or millions of times, using many training batches, until the model parame‐ ters converge to values that (hopefully) minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff (although it wasn’t called that when backpropagation was invented, and it has been reinvented sev‐ eral times). Reverse-mode autodiff performs a forward pass through a computa‐ tion graph, computing every node’s value for the current training batch, and then it performs a reverse pass, computing all the gradients at once (see Appendix D for more details). So what’s the difference? Well, backpropagation refers to the whole process of training an artificial neural network using multiple backpropa‐ gation steps, each of which computes gradients and uses them to perform a Gra‐ dient Descent step. In contrast, reverse-mode autodiff is a simply a technique to compute gradients efficiently, and it happens to be used by backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of chapter 9, try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28;\n",
    "n_hidden1 = 300;\n",
    "n_hidden2 = 100;\n",
    "n_outputs = 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used Variables to manage our data, but there is a more basic structure, the ***placeholder***. A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlowterminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "#X_train = X_train.reshape(-1, 28*28);\n",
    "#X_test  = X_test.reshape(-1, 28*28);\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.Dense(300, activation=\"relu\",),\n",
    "        keras.layers.Dense(100, activation=\"relu\",),\n",
    "        keras.layers.Dense(10, activation=\"relu\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (60000, 1) was passed for an output of shape (None, 10) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-d9f7d81b1ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    808\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    809\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (60000, 1) was passed for an output of shape (None, 10) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
