\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Preliminary definitions}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Trace of a matrix}{1}{subsection.1.1}}
\newlabel{trace}{{1.1}{1}{Trace of a matrix}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Expectation}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Variance}{2}{subsection.1.3}}
\newlabel{Variance}{{1.3}{2}{Variance}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Median}{2}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Median as the minimizer of $L_1$ norm}{3}{subsection.1.5}}
\newlabel{medianMin}{{1.5}{3}{Median as the minimizer of $L_1$ norm}{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Gaussian function and gaussian distribution}{7}{subsection.1.6}}
\newlabel{gaussian}{{1.6}{7}{Gaussian function and gaussian distribution}{subsection.1.6}{}}
\newlabel{gauss2}{{1.17}{7}{}{equation.1.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Statistical Decision Theory}{8}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Expected prediction error}{8}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Loss function for categorical variables}{11}{subsubsection.2.1.1}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Note}: When models or loss functions use additional parameters that penalize complexity (Lasso, Ridge and others) we cannot use the training data to determine these parameters, since we would pick those that gave interpolating fits with zero residuals but it will be unlikely to predict future data.}{12}{section*.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Bias-Variance trade-off}{12}{subsection.2.2}}
\newlabel{biasVar}{{2.16}{12}{Bias-Variance trade-off}{equation.2.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Linear Regression Models}{13}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Univariate linear regression}{13}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Equivalence of Ordinary least squares and maximum likelihood}{14}{subsection.3.2}}
\newlabel{multiGauss}{{3.9}{14}{Equivalence of Ordinary least squares and maximum likelihood}{equation.3.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition of likelihood}{15}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Expectation of the parameter estimation: unbiased estimator}{16}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Noise variance estimation}{17}{subsection.3.4}}
\newlabel{varErr}{{3.22}{17}{Noise variance estimation}{equation.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Interpretation of covariance}{20}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Z-score}{20}{subsection.3.6}}
\newlabel{parameterVar}{{3.28}{20}{Z-score}{equation.3.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Orthogonalization}{22}{subsection.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Multivariate output}{23}{subsection.3.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Subset selection}{23}{subsection.3.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.1}Best subset selection}{24}{subsubsection.3.9.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.2}Forward stepwise selection}{24}{subsubsection.3.9.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.3}Backward stepwise selection}{24}{subsubsection.3.9.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.4}Implementations}{24}{subsubsection.3.9.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.9.5}Forward stagewise regression}{25}{subsubsection.3.9.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10}Shrinkage methods}{25}{subsection.3.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1}Ridge regression}{25}{subsubsection.3.10.1}}
\newlabel{bridge}{{3.40}{25}{Ridge regression}{equation.3.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.2}Lasso regression}{27}{subsubsection.3.10.2}}
\newlabel{blasso}{{3.46}{27}{Lasso regression}{equation.3.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.3}Least angle regression}{27}{subsubsection.3.10.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Constraints of ridge and lasso regression.\relax }}{28}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ridge_lasso}{{3.1}{28}{Constraints of ridge and lasso regression.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Shapes of different penalization factors.\relax }}{28}{figure.caption.5}}
\newlabel{penalization}{{3.2}{28}{Shapes of different penalization factors.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11}Derived input directions methods}{29}{subsection.3.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.1}Principal component analysis}{30}{subsubsection.3.11.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.11.2}Partial least squares}{30}{subsubsection.3.11.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12}Multioutput shrinkage and selection}{30}{subsection.3.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.13}Other derived algorithms}{31}{subsection.3.13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.13.1}Incremental forward stagewise}{31}{subsubsection.3.13.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.13.2}The Dantzig selector}{31}{subsubsection.3.13.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.13.3}The Grouped Lasso}{32}{subsubsection.3.13.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Bayesian inference}{32}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction to Bayes' theorem}{32}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Bayes inference}{33}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Types of estimation}{33}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Conjugate distributions}{34}{subsection.4.4}}
\newlabel{conjugacy}{{4.4}{34}{Conjugate distributions}{subsection.4.4}{}}
\newlabel{ClassBayes}{{4.4}{35}{Conjugate distributions}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Dataset likelihood}{35}{subsubsection.4.4.1}}
\newlabel{dataset likelihood}{{4.4.1}{35}{Dataset likelihood}{subsubsection.4.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Linear Classification}{36}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Linear regression of an Indicator matrix}{36}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Linear Discriminant analysis}{37}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Decision rule}{39}{subsubsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Quadratic Discriminant analysis}{39}{subsection.5.3}}
\newlabel{QDA}{{5.11}{39}{Quadratic Discriminant analysis}{equation.5.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Regularized discriminant analysis}{40}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Computation}{40}{subsection.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces ...\relax }}{41}{figure.caption.9}}
\newlabel{proj}{{5.1}{41}{...\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Regularized-rank linear discriminant analysis}{41}{subsection.5.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Covariance.\relax }}{42}{figure.caption.10}}
\newlabel{cov}{{5.2}{42}{Covariance.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Logistic regression}{43}{subsection.5.7}}
\newlabel{logit}{{5.19}{43}{Logistic regression}{equation.5.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.1}Multinomial logistic regression: more than 2 classes}{44}{subsubsection.5.7.1}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Softmax function}}{44}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.2}Fitting logistic regression}{45}{subsubsection.5.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Newton-Raphsone example.\relax }}{47}{figure.caption.13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.3}First method: point estimation, the MAP solution}{47}{subsubsection.5.7.3}}
\@writefile{toc}{\contentsline {paragraph}{The Newton-Raphson method}{47}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Derivation}{48}{section*.14}}
\newlabel{devg}{{5.36}{49}{Derivation}{equation.5.36}{}}
\newlabel{devPn}{{5.37}{49}{Derivation}{equation.5.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.4}Second method: Laplace approximation}{53}{subsubsection.5.7.4}}
\newlabel{Taylor}{{5.50}{53}{Second method: Laplace approximation}{equation.5.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Example of multinomial Gaussian.\relax }}{54}{figure.caption.15}}
\newlabel{gaussianfig}{{5.4}{54}{Example of multinomial Gaussian.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The axis are the parameters. The Laplace approximation of the posterior (darker lines) is shown while the lighter lines are the true unnormalised posterior. The centre point corresponds to the maximum point $\mathaccentV {hat}05E{\Mathbf  {\beta }}$.\relax }}{55}{figure.caption.16}}
\newlabel{posteriorApprox}{{5.5}{55}{The axis are the parameters. The Laplace approximation of the posterior (darker lines) is shown while the lighter lines are the true unnormalised posterior. The centre point corresponds to the maximum point $\hbe $.\relax }{figure.caption.16}{}}
\newlabel{approxDen}{{5.55}{55}{Second method: Laplace approximation}{equation.5.55}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Comparison between the decision boundaries of MAP and Laplace method.}}{55}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.5}Third method: sampling technique }{56}{subsubsection.5.7.5}}
\@writefile{toc}{\contentsline {paragraph}{The metropolis-Hastings algorithms}{56}{section*.19}}
\newlabel{MAP1}{{5.6a}{57}{Decision boundary for MAP estimation.\relax }{figure.caption.18}{}}
\newlabel{sub@MAP1}{{a}{57}{Decision boundary for MAP estimation.\relax }{figure.caption.18}{}}
\newlabel{laplace1}{{5.6b}{57}{Decision boundary for Laplace approximation estimation.\relax }{figure.caption.18}{}}
\newlabel{sub@laplace1}{{b}{57}{Decision boundary for Laplace approximation estimation.\relax }{figure.caption.18}{}}
\newlabel{MAP2}{{5.6c}{57}{Contours of probability of belonging to class 1 in case of MAP estimation.\relax }{figure.caption.18}{}}
\newlabel{sub@MAP2}{{c}{57}{Contours of probability of belonging to class 1 in case of MAP estimation.\relax }{figure.caption.18}{}}
\newlabel{laplace2}{{5.6d}{57}{Contours of probability of belonging to class 1 in case of Laplace approximation estimation computed with a sample based approximation.\relax }{figure.caption.18}{}}
\newlabel{sub@laplace2}{{d}{57}{Contours of probability of belonging to class 1 in case of Laplace approximation estimation computed with a sample based approximation.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison between MAP and Laplace approximation contours and probability densities.\relax }}{57}{figure.caption.18}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {The generation of new samples}}{58}{section*.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The steps of the Metropolis-Hastings algorithm. $U$ is a uniform distribution in $[0,1]$\relax }}{59}{figure.caption.21}}
\newlabel{MHAlgo}{{5.7}{59}{The steps of the Metropolis-Hastings algorithm. $U$ is a uniform distribution in $[0,1]$\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Example of Metropolis-Hastings iterations: solid lines are accepted coefficients, dashed lines are the rejected ones.\relax }}{60}{figure.caption.22}}
\newlabel{randomWalk}{{5.8}{60}{Example of Metropolis-Hastings iterations: solid lines are accepted coefficients, dashed lines are the rejected ones.\relax }{figure.caption.22}{}}
\newlabel{MH1}{{5.9a}{61}{Predictive probability contours of classifying objects as square.\relax }{figure.caption.24}{}}
\newlabel{sub@MH1}{{a}{61}{Predictive probability contours of classifying objects as square.\relax }{figure.caption.24}{}}
\newlabel{MH2}{{5.9b}{61}{Decision boundaries created from randomly selected MH samples.\relax }{figure.caption.24}{}}
\newlabel{sub@MH2}{{b}{61}{Decision boundaries created from randomly selected MH samples.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Example of Metropolis-Hastings algorithm results.\relax }}{61}{figure.caption.24}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {To calculate the predictive probability}}{61}{section*.23}}
\newlabel{mode1}{{5.10a}{62}{A bi-modal density.\relax }{figure.caption.26}{}}
\newlabel{sub@mode1}{{a}{62}{A bi-modal density.\relax }{figure.caption.26}{}}
\newlabel{mode2}{{5.10b}{62}{Highly correlated parameter density.\relax }{figure.caption.26}{}}
\newlabel{sub@mode2}{{b}{62}{Highly correlated parameter density.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Example of Metropolis-Hastings algorithm results.\relax }}{62}{figure.caption.26}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Limitations}}{62}{section*.25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.6}Usage}{62}{subsubsection.5.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Regularized Logistic regression}{63}{subsection.5.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Logistic vs LDA}{63}{subsection.5.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}Perceptron learning algorithm}{63}{subsection.5.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.11}Optimal separating hyperplanes}{64}{subsection.5.11}}
\newlabel{eq}{{5.72}{64}{Optimal separating hyperplanes}{equation.5.72}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Basis expansion and regularization}{65}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Piecewise Polynomials and splines}{65}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Linear splines. The solid line is the global function while the dashed lines are the basis functions.\relax }}{69}{figure.caption.28}}
\newlabel{spline_linear}{{6.1}{69}{Linear splines. The solid line is the global function while the dashed lines are the basis functions.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Natural cubic splines}{69}{subsection.6.2}}
\newlabel{spline_cubic}{{\caption@xref {spline_cubic}{ on input line 1930}}{71}{Piecewise Polynomials and splines}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Cubic splines. The solid line is the global function while the dashed lines are the basis functions.\relax }}{71}{figure.caption.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces $X$ consists of 50 points drawn randomly from $U[0,1]$ and an assumed error model of with constant variance. Both cubic splines have 6 degrees of freedom. The cubic spline has two knots at $0.33$ and at $0.66$ while the natural cubic spline has boundary knots at $0.1$ and $0.9$, and four interior knots uniformly spaced between them.\relax }}{72}{figure.caption.31}}
\newlabel{pointwise_var}{{6.3}{72}{$X$ consists of 50 points drawn randomly from $U[0,1]$ and an assumed error model of with constant variance. Both cubic splines have 6 degrees of freedom. The cubic spline has two knots at $0.33$ and at $0.66$ while the natural cubic spline has boundary knots at $0.1$ and $0.9$, and four interior knots uniformly spaced between them.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Smoothing splines}{75}{subsection.6.3}}
\newlabel{RSSspl}{{6.16}{76}{Smoothing splines}{equation.6.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Multidimensional splines}{77}{subsection.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces In green the fitting curves. On the left the one resulting from \textit  {k-nearest neighbourhood}, on the right the metric neighbourhood.\relax }}{79}{figure.caption.32}}
\newlabel{nearest}{{7.1}{79}{In green the fitting curves. On the left the one resulting from \textit {k-nearest neighbourhood}, on the right the metric neighbourhood.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Wavelet smoothing}{79}{subsection.6.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Kernel smoothing methods}{79}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}One-dimensional kernel smoothers}{79}{subsection.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces A comparison of three popular kernels for local smoothing. Each has been calibrated to integrate to 1. The tri-cube kernel is compact and has two continuous derivatives at the boundary of its support, while the Epanechnikov ker- nel has none. The Gaussian kernel is continuously differentiable, but has infinite support.\relax }}{81}{figure.caption.33}}
\newlabel{kernels}{{7.2}{81}{A comparison of three popular kernels for local smoothing. Each has been calibrated to integrate to 1. The tri-cube kernel is compact and has two continuous derivatives at the boundary of its support, while the Epanechnikov ker- nel has none. The Gaussian kernel is continuously differentiable, but has infinite support.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Local linear regression}{81}{subsubsection.7.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces On the left it can be noticed the bias effect at the boundaries.\relax }}{82}{figure.caption.34}}
\newlabel{biasedKernel}{{7.3}{82}{On the left it can be noticed the bias effect at the boundaries.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Local polynomial regression}{82}{subsubsection.7.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Local linear fits exhibit bias in regions of curvature of the true function. Local quadratic fits tend to eliminate this bias.\relax }}{83}{figure.caption.35}}
\newlabel{localLinQuad}{{7.4}{83}{Local linear fits exhibit bias in regions of curvature of the true function. Local quadratic fits tend to eliminate this bias.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Local regression in $\mathcal  {R}^p$}{83}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Structured local regression models in $\mathcal  {R}^p$}{84}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Structured kernels}{84}{subsubsection.7.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Structured regression function}{84}{subsubsection.7.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Local likelihood and other models}{85}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Local multiclassifier linear logistic regression}{85}{subsubsection.7.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Kernel density classification}{86}{subsection.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Naive-Bayes classifier}{86}{subsection.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Radial basis functions}{86}{subsection.7.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8}Mixture Models for Density Estimation and Classification}{87}{subsection.7.8}}
\newlabel{mix}{{7.21}{87}{Mixture Models for Density Estimation and Classification}{equation.7.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Model assessment and selection}{88}{section.8}}
\newlabel{trainErr}{{8.1}{88}{}{equation.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Behaviour of test sample and training sample error as the model complexity is varied. The light blue curves show the training error $\mathaccentV {bar}016{err}$, while the light red curves show the conditional test error $Err_\tau $ for $100$ training sets of size $50$ each, as the model complexity is increased. The solid curves show the expected test error $Err$ and the expected training error $E[\mathaccentV {bar}016{err}]$.\relax }}{89}{figure.caption.36}}
\newlabel{testErr}{{8.1}{89}{Behaviour of test sample and training sample error as the model complexity is varied. The light blue curves show the training error $\bar {err}$, while the light red curves show the conditional test error $Err_\tau $ for $100$ training sets of size $50$ each, as the model complexity is increased. The solid curves show the expected test error $Err$ and the expected training error $E[\bar {err}]$.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Optimism of the training error rate}{91}{subsection.8.1}}
\newlabel{paramSel}{{8.14}{92}{Optimism of the training error rate}{equation.8.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}$C_p$ metric}{93}{subsection.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Akaike Information Criterion}{93}{subsection.8.3}}
\newlabel{varMul}{{8.28}{95}{Akaike Information Criterion}{equation.8.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Effective number of parameters}{97}{subsection.8.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Bayesian Information Criterion}{98}{subsection.8.5}}
\newlabel{BIC}{{8.49}{98}{Bayesian Information Criterion}{equation.8.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Minimum Description Length}{99}{subsection.8.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Cross-Validation}{99}{subsection.8.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.7.1}K-fold cross-validation}{99}{subsubsection.8.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Bootstrap Methods}{100}{subsection.8.8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Additive models}{101}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Tree based methods}{101}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Regression Trees}{101}{subsubsection.9.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces 2D-input space binary tree partition.\relax }}{102}{figure.caption.37}}
\newlabel{tree1}{{9.1}{102}{2D-input space binary tree partition.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Another representation of the binary tree on the left. On the right the 3D resulting shape of the function.\relax }}{102}{figure.caption.38}}
\newlabel{tree1}{{9.2}{102}{Another representation of the binary tree on the left. On the right the 3D resulting shape of the function.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Classification trees}{104}{subsection.9.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Linear Combination splits}{104}{subsubsection.9.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Limitations of trees}{105}{subsubsection.9.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Patient rule induction method (PRIM)}{105}{subsection.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Multivariate Adaptive Regression Splines}{106}{subsection.9.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Steps in PRIM.\relax }}{107}{figure.caption.40}}
