\BOOKMARK [1][-]{section.1}{Preliminary definitions}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Trace of a matrix}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Expectation}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Variance}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Median}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Median as the minimizer of L1 norm}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{Gaussian function and gaussian distribution}{section.1}% 7
\BOOKMARK [1][-]{section.2}{Statistical Decision Theory}{}% 8
\BOOKMARK [2][-]{subsection.2.1}{Expected prediction error}{section.2}% 9
\BOOKMARK [3][-]{subsubsection.2.1.1}{Loss function for categorical variables}{subsection.2.1}% 10
\BOOKMARK [2][-]{subsection.2.2}{Bias-Variance trade-off}{section.2}% 11
\BOOKMARK [1][-]{section.3}{Linear Regression Models}{}% 12
\BOOKMARK [2][-]{subsection.3.1}{Univariate linear regression}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.2}{Equivalence of Ordinary least squares and maximum likelihood}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.3}{Expectation of the parameter estimation: unbiased estimator}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.4}{Noise variance estimation}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.5}{Interpretation of covariance}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.6}{Z-score}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.7}{Orthogonalization}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.8}{Multivariate output}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.9}{Subset selection}{section.3}% 21
\BOOKMARK [3][-]{subsubsection.3.9.1}{Best subset selection}{subsection.3.9}% 22
\BOOKMARK [3][-]{subsubsection.3.9.2}{Forward stepwise selection}{subsection.3.9}% 23
\BOOKMARK [3][-]{subsubsection.3.9.3}{Backward stepwise selection}{subsection.3.9}% 24
\BOOKMARK [3][-]{subsubsection.3.9.4}{Implementations}{subsection.3.9}% 25
\BOOKMARK [3][-]{subsubsection.3.9.5}{Forward stagewise regression}{subsection.3.9}% 26
\BOOKMARK [2][-]{subsection.3.10}{Shrinkage methods}{section.3}% 27
\BOOKMARK [3][-]{subsubsection.3.10.1}{Ridge regression}{subsection.3.10}% 28
\BOOKMARK [3][-]{subsubsection.3.10.2}{Lasso regression}{subsection.3.10}% 29
\BOOKMARK [3][-]{subsubsection.3.10.3}{Least angle regression}{subsection.3.10}% 30
\BOOKMARK [2][-]{subsection.3.11}{Derived input directions methods}{section.3}% 31
\BOOKMARK [3][-]{subsubsection.3.11.1}{Principal component analysis}{subsection.3.11}% 32
\BOOKMARK [3][-]{subsubsection.3.11.2}{Partial least squares}{subsection.3.11}% 33
\BOOKMARK [2][-]{subsection.3.12}{Multioutput shrinkage and selection}{section.3}% 34
\BOOKMARK [2][-]{subsection.3.13}{Other derived algorithms}{section.3}% 35
\BOOKMARK [3][-]{subsubsection.3.13.1}{Incremental forward stagewise}{subsection.3.13}% 36
\BOOKMARK [3][-]{subsubsection.3.13.2}{The Dantzig selector}{subsection.3.13}% 37
\BOOKMARK [3][-]{subsubsection.3.13.3}{The Grouped Lasso}{subsection.3.13}% 38
\BOOKMARK [1][-]{section.4}{Bayesian inference}{}% 39
\BOOKMARK [2][-]{subsection.4.1}{Introduction to Bayes' theorem}{section.4}% 40
\BOOKMARK [2][-]{subsection.4.2}{Bayes inference}{section.4}% 41
\BOOKMARK [2][-]{subsection.4.3}{Types of estimation}{section.4}% 42
\BOOKMARK [2][-]{subsection.4.4}{Conjugate distributions}{section.4}% 43
\BOOKMARK [3][-]{subsubsection.4.4.1}{Dataset likelihood}{subsection.4.4}% 44
\BOOKMARK [1][-]{section.5}{Linear Classification}{}% 45
\BOOKMARK [2][-]{subsection.5.1}{Linear regression of an Indicator matrix}{section.5}% 46
\BOOKMARK [2][-]{subsection.5.2}{Linear Discriminant analysis}{section.5}% 47
\BOOKMARK [3][-]{subsubsection.5.2.1}{Decision rule}{subsection.5.2}% 48
\BOOKMARK [2][-]{subsection.5.3}{Quadratic Discriminant analysis}{section.5}% 49
\BOOKMARK [2][-]{subsection.5.4}{Regularized discriminant analysis}{section.5}% 50
\BOOKMARK [2][-]{subsection.5.5}{Computation}{section.5}% 51
\BOOKMARK [2][-]{subsection.5.6}{Regularized-rank linear discriminant analysis}{section.5}% 52
\BOOKMARK [2][-]{subsection.5.7}{Logistic regression}{section.5}% 53
\BOOKMARK [3][-]{subsubsection.5.7.1}{Multinomial logistic regression: more than 2 classes}{subsection.5.7}% 54
\BOOKMARK [3][-]{subsubsection.5.7.2}{Fitting logistic regression}{subsection.5.7}% 55
\BOOKMARK [3][-]{subsubsection.5.7.3}{First method: point estimation, the MAP solution}{subsection.5.7}% 56
\BOOKMARK [3][-]{subsubsection.5.7.4}{Second method: Laplace approximation}{subsection.5.7}% 57
\BOOKMARK [3][-]{subsubsection.5.7.5}{Third method: sampling technique }{subsection.5.7}% 58
\BOOKMARK [3][-]{subsubsection.5.7.6}{Usage}{subsection.5.7}% 59
\BOOKMARK [2][-]{subsection.5.8}{Regularized Logistic regression}{section.5}% 60
\BOOKMARK [2][-]{subsection.5.9}{Logistic vs LDA}{section.5}% 61
\BOOKMARK [2][-]{subsection.5.10}{Perceptron learning algorithm}{section.5}% 62
\BOOKMARK [2][-]{subsection.5.11}{Optimal separating hyperplanes}{section.5}% 63
\BOOKMARK [1][-]{section.6}{Basis expansion and regularization}{}% 64
\BOOKMARK [2][-]{subsection.6.1}{Piecewise Polynomials and splines}{section.6}% 65
\BOOKMARK [2][-]{subsection.6.2}{Natural cubic splines}{section.6}% 66
\BOOKMARK [2][-]{subsection.6.3}{Smoothing splines}{section.6}% 67
\BOOKMARK [2][-]{subsection.6.4}{Multidimensional splines}{section.6}% 68
\BOOKMARK [2][-]{subsection.6.5}{Wavelet smoothing}{section.6}% 69
\BOOKMARK [1][-]{section.7}{Kernel smoothing methods}{}% 70
\BOOKMARK [2][-]{subsection.7.1}{One-dimensional kernel smoothers}{section.7}% 71
\BOOKMARK [3][-]{subsubsection.7.1.1}{Local linear regression}{subsection.7.1}% 72
\BOOKMARK [3][-]{subsubsection.7.1.2}{Local polynomial regression}{subsection.7.1}% 73
\BOOKMARK [2][-]{subsection.7.2}{Local regression in Rp}{section.7}% 74
\BOOKMARK [2][-]{subsection.7.3}{Structured local regression models in Rp}{section.7}% 75
\BOOKMARK [3][-]{subsubsection.7.3.1}{Structured kernels}{subsection.7.3}% 76
\BOOKMARK [3][-]{subsubsection.7.3.2}{Structured regression function}{subsection.7.3}% 77
\BOOKMARK [2][-]{subsection.7.4}{Local likelihood and other models}{section.7}% 78
\BOOKMARK [3][-]{subsubsection.7.4.1}{Local multiclassifier linear logistic regression}{subsection.7.4}% 79
\BOOKMARK [2][-]{subsection.7.5}{Kernel density classification}{section.7}% 80
\BOOKMARK [2][-]{subsection.7.6}{Naive-Bayes classifier}{section.7}% 81
\BOOKMARK [2][-]{subsection.7.7}{Radial basis functions}{section.7}% 82
\BOOKMARK [2][-]{subsection.7.8}{Mixture Models for Density Estimation and Classification}{section.7}% 83
\BOOKMARK [1][-]{section.8}{Model assessment and selection}{}% 84
\BOOKMARK [2][-]{subsection.8.1}{Optimism of the training error rate}{section.8}% 85
\BOOKMARK [2][-]{subsection.8.2}{Cp metric}{section.8}% 86
\BOOKMARK [2][-]{subsection.8.3}{Akaike Information Criterion}{section.8}% 87
\BOOKMARK [2][-]{subsection.8.4}{Effective number of parameters}{section.8}% 88
\BOOKMARK [2][-]{subsection.8.5}{Bayesian Information Criterion}{section.8}% 89
\BOOKMARK [2][-]{subsection.8.6}{Minimum Description Length}{section.8}% 90
\BOOKMARK [2][-]{subsection.8.7}{Cross-Validation}{section.8}% 91
\BOOKMARK [3][-]{subsubsection.8.7.1}{K-fold cross-validation}{subsection.8.7}% 92
\BOOKMARK [2][-]{subsection.8.8}{Bootstrap Methods}{section.8}% 93
\BOOKMARK [1][-]{section.9}{Additive models}{}% 94
\BOOKMARK [2][-]{subsection.9.1}{Tree based methods}{section.9}% 95
\BOOKMARK [3][-]{subsubsection.9.1.1}{Regression Trees}{subsection.9.1}% 96
\BOOKMARK [2][-]{subsection.9.2}{Classification trees}{section.9}% 97
\BOOKMARK [3][-]{subsubsection.9.2.1}{Linear Combination splits}{subsection.9.2}% 98
\BOOKMARK [3][-]{subsubsection.9.2.2}{Limitations of trees}{subsection.9.2}% 99
\BOOKMARK [2][-]{subsection.9.3}{Patient rule induction method \(PRIM\)}{section.9}% 100
\BOOKMARK [2][-]{subsection.9.4}{Multivariate Adaptive Regression Splines}{section.9}% 101
